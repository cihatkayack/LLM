{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cihatkayack/LLM/blob/main/Exercise_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0HEeSjnWHMK"
      },
      "source": [
        "# Exercise 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dkrIsWpWF4J",
        "outputId": "0971faa6-e6f5-4d6b-dd49-e220f18e071c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-25 14:36:15--  https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20479 (20K) [text/plain]\n",
            "Saving to: ‘verdict.txt’\n",
            "\n",
            "\r",
            "verdict.txt           0%[                    ]       0  --.-KB/s               \r",
            "verdict.txt         100%[===================>]  20.00K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-04-25 14:36:15 (14.6 MB/s) - ‘verdict.txt’ saved [20479/20479]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt \\\n",
        "     -O verdict.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Miv5cc_WPja"
      },
      "outputs": [],
      "source": [
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# 1.\n",
        "bpe_tokenizer = Tokenizer(BPE())\n",
        "bpe_tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# 2.\n",
        "bpe_trainer = BpeTrainer(vocab_size=1000)\n",
        "\n",
        "# 3.\n",
        "with open(\"verdict.txt\", 'r', encoding='utf-8') as f:\n",
        "    training_data = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "# 4.\n",
        "bpe_tokenizer.train_from_iterator(training_data, bpe_trainer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIFVdOeqWPgp",
        "outputId": "75cb38ec-4514-4229-af1f-3020450d2c2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'white': 933,\n",
              " 'ince': 507,\n",
              " 'cheeks': 961,\n",
              " 'ever': 182,\n",
              " 'ame': 576,\n",
              " 'ant': 270,\n",
              " 'wonder': 711,\n",
              " 'j': 43,\n",
              " 'pat': 810,\n",
              " 'light': 336,\n",
              " 'alities': 906,\n",
              " 'de': 101,\n",
              " 'age': 536,\n",
              " 'painter': 925,\n",
              " 'io': 482,\n",
              " 'llow': 443,\n",
              " 'T': 28,\n",
              " 'tor': 869,\n",
              " 'bec': 246,\n",
              " 'says': 940,\n",
              " 'You': 575,\n",
              " 'bear': 879,\n",
              " 'called': 726,\n",
              " 'lt': 321,\n",
              " 'sh': 164,\n",
              " 'pictures': 397,\n",
              " 'Her': 566,\n",
              " 'giv': 362,\n",
              " 'is': 69,\n",
              " 'wh': 174,\n",
              " 'int': 379,\n",
              " 'king': 242,\n",
              " 'thre': 525,\n",
              " 'cont': 968,\n",
              " 'gen': 790,\n",
              " 'as': 71,\n",
              " 'fied': 789,\n",
              " 'g': 40,\n",
              " 'Well': 351,\n",
              " 'ps': 299,\n",
              " 'wouldn': 686,\n",
              " ')': 4,\n",
              " 'work': 403,\n",
              " 'cor': 474,\n",
              " 'bor': 759,\n",
              " 'eigh': 782,\n",
              " 'sis': 820,\n",
              " 'usu': 839,\n",
              " 'rest': 332,\n",
              " 'it': 68,\n",
              " 'old': 491,\n",
              " 'co': 215,\n",
              " 'ction': 675,\n",
              " 'ghed': 792,\n",
              " '(': 3,\n",
              " '\"--': 563,\n",
              " 'Miss': 570,\n",
              " 'half': 510,\n",
              " 'even': 532,\n",
              " 'mantel': 950,\n",
              " 'in': 61,\n",
              " 'ople': 620,\n",
              " 'told': 383,\n",
              " 'nex': 533,\n",
              " 'ple': 300,\n",
              " 'canvas': 978,\n",
              " 'object': 492,\n",
              " 'gged': 714,\n",
              " 'face': 398,\n",
              " 'get': 361,\n",
              " 'ity': 232,\n",
              " 'wing': 329,\n",
              " 'own': 161,\n",
              " 'poor': 537,\n",
              " 'igh': 604,\n",
              " 'if': 218,\n",
              " ',': 5,\n",
              " 'husband': 728,\n",
              " 'nic': 806,\n",
              " 'alous': 904,\n",
              " 'things': 683,\n",
              " 'vind': 843,\n",
              " 'O': 24,\n",
              " 'here': 504,\n",
              " 'leas': 877,\n",
              " 'satis': 560,\n",
              " 'herself': 672,\n",
              " 'over': 619,\n",
              " 'ddenly': 707,\n",
              " 'R': 26,\n",
              " 'won': 433,\n",
              " 'ef': 476,\n",
              " 'brou': 761,\n",
              " 'bru': 762,\n",
              " 'tr': 628,\n",
              " 'irre': 908,\n",
              " 'laughed': 986,\n",
              " 'W': 31,\n",
              " 'Don': 741,\n",
              " 'C': 13,\n",
              " 'above': 528,\n",
              " 'rid': 815,\n",
              " 'ious': 293,\n",
              " 'art': 272,\n",
              " 'me': 86,\n",
              " 'pt': 494,\n",
              " 'Riv': 572,\n",
              " 'tic': 631,\n",
              " 'jealous': 973,\n",
              " 'es': 103,\n",
              " 'dra': 356,\n",
              " 'ry_': 975,\n",
              " 'mia': 938,\n",
              " 'ier': 603,\n",
              " 'ru': 211,\n",
              " 'slight': 825,\n",
              " 'ang': 643,\n",
              " '_was': 748,\n",
              " 'deep': 895,\n",
              " 'their': 381,\n",
              " 'round': 912,\n",
              " 'medi': 974,\n",
              " 'ral': 898,\n",
              " 'brown': 981,\n",
              " 'only': 245,\n",
              " 'un': 303,\n",
              " 'you': 128,\n",
              " 'sur': 262,\n",
              " 'sketch': 561,\n",
              " 'grew': 954,\n",
              " 'burn': 158,\n",
              " 'found': 409,\n",
              " 'colour': 947,\n",
              " 'ou': 64,\n",
              " 'tel': 668,\n",
              " 'subject': 916,\n",
              " '--\"': 651,\n",
              " 'cul': 768,\n",
              " 'cr': 584,\n",
              " 'terrace': 922,\n",
              " 'stro': 385,\n",
              " 'himself': 275,\n",
              " 'wit': 115,\n",
              " 'thou': 249,\n",
              " 's': 52,\n",
              " 'be': 84,\n",
              " 'show': 340,\n",
              " 'dead': 388,\n",
              " 'cre': 473,\n",
              " 'af': 283,\n",
              " 'eks': 553,\n",
              " 'began': 556,\n",
              " 'great': 545,\n",
              " 'tu': 829,\n",
              " 'az': 752,\n",
              " 'ek': 357,\n",
              " 'room': 276,\n",
              " 'anc': 382,\n",
              " 'to': 72,\n",
              " 'G': 17,\n",
              " 'Whe': 468,\n",
              " 'between': 406,\n",
              " 'how': 363,\n",
              " 'understand': 967,\n",
              " 'ried': 400,\n",
              " 'inev': 854,\n",
              " 'exquisite': 949,\n",
              " 'do': 255,\n",
              " 'drawing': 699,\n",
              " 'tru': 835,\n",
              " 'lled': 887,\n",
              " 'app': 756,\n",
              " 'sa': 160,\n",
              " 'alone': 903,\n",
              " 'stu': 870,\n",
              " 'gre': 241,\n",
              " 'hand': 305,\n",
              " 'out': 136,\n",
              " 'The': 157,\n",
              " 'o': 48,\n",
              " 'hung': 796,\n",
              " 'od': 221,\n",
              " 'alway': 390,\n",
              " 'rich': 816,\n",
              " 'ary': 883,\n",
              " 'che': 253,\n",
              " 'left': 654,\n",
              " 'where': 453,\n",
              " 'enly': 649,\n",
              " '--': 78,\n",
              " 'ween': 393,\n",
              " 'ming': 801,\n",
              " 'teen': 669,\n",
              " 'gh': 100,\n",
              " 'Th': 467,\n",
              " 'last': 402,\n",
              " 'so': 130,\n",
              " 'fas': 785,\n",
              " 'que': 928,\n",
              " 'ra': 105,\n",
              " 'rose': 661,\n",
              " 'ser': 821,\n",
              " 'hav': 509,\n",
              " 'on': 67,\n",
              " 'enough': 439,\n",
              " 'paint': 162,\n",
              " 'cing': 585,\n",
              " 'su': 127,\n",
              " 'sm': 817,\n",
              " 'sim': 431,\n",
              " 'cus': 475,\n",
              " 'der': 594,\n",
              " 'lift': 665,\n",
              " 'looked': 456,\n",
              " 'gro': 791,\n",
              " 'degre': 894,\n",
              " 'hard': 639,\n",
              " 'iron': 664,\n",
              " 'died': 779,\n",
              " 'ting': 263,\n",
              " '-': 6,\n",
              " 'ucked': 633,\n",
              " 'sol': 920,\n",
              " 'N': 23,\n",
              " 'watching': 461,\n",
              " 'suddenly': 917,\n",
              " '!\"': 349,\n",
              " 'M': 22,\n",
              " ';': 9,\n",
              " 'li': 118,\n",
              " 'pected': 541,\n",
              " 'little': 405,\n",
              " 'win': 847,\n",
              " 'key': 391,\n",
              " 'displ': 941,\n",
              " 'Rick': 316,\n",
              " 'given': 411,\n",
              " 'f': 39,\n",
              " 'pan': 495,\n",
              " 'bar': 760,\n",
              " 'an': 66,\n",
              " 'turn': 432,\n",
              " 'led': 609,\n",
              " 'Gis': 163,\n",
              " 'smi': 498,\n",
              " 'ality': 663,\n",
              " 'ping': 621,\n",
              " 'If': 567,\n",
              " 'ould': 126,\n",
              " 'tech': 913,\n",
              " 'weeks': 923,\n",
              " 'fir': 360,\n",
              " 'etly': 944,\n",
              " 'ome': 809,\n",
              " 'delicate': 732,\n",
              " 'point': 538,\n",
              " 'v': 55,\n",
              " 'there': 304,\n",
              " 'ut': 112,\n",
              " 'fo': 217,\n",
              " 'through': 282,\n",
              " 'picture': 183,\n",
              " 'note': 445,\n",
              " 'wife': 460,\n",
              " 'come': 684,\n",
              " 'ouched': 857,\n",
              " 'she': 155,\n",
              " 'U': 29,\n",
              " 'ke': 134,\n",
              " 'ard': 247,\n",
              " 'ham': 331,\n",
              " 'ations': 399,\n",
              " 'call': 589,\n",
              " 'us': 185,\n",
              " 'for': 140,\n",
              " 'ared': 754,\n",
              " 'pal': 812,\n",
              " 'stay': 871,\n",
              " 'eved': 921,\n",
              " ':': 8,\n",
              " 'ey': 238,\n",
              " 'br': 352,\n",
              " 'fe': 129,\n",
              " 'moved': 535,\n",
              " 'Rivier': 724,\n",
              " 'x': 57,\n",
              " 'ure': 147,\n",
              " 'ul': 265,\n",
              " 'question': 733,\n",
              " 'um': 327,\n",
              " 'ie': 481,\n",
              " 'liked': 909,\n",
              " 'shru': 927,\n",
              " 'ry': 301,\n",
              " 'since': 627,\n",
              " 'fro': 319,\n",
              " 'yet': 850,\n",
              " 'hus': 601,\n",
              " 'dd': 419,\n",
              " 'er': 76,\n",
              " 'ies': 605,\n",
              " 'Poor': 723,\n",
              " 'S': 27,\n",
              " 'pris': 459,\n",
              " 'arm': 386,\n",
              " 'saw': 449,\n",
              " 'mar': 488,\n",
              " 'cle': 586,\n",
              " 'ish': 647,\n",
              " 'sing': 496,\n",
              " 'cha': 285,\n",
              " 'his': 89,\n",
              " 'been': 175,\n",
              " 'who': 280,\n",
              " 'vas': 634,\n",
              " 'again': 343,\n",
              " 'rent': 641,\n",
              " 'pr': 298,\n",
              " 'could': 202,\n",
              " 'ath': 335,\n",
              " 'ling': 322,\n",
              " 'resting': 551,\n",
              " 'mur': 802,\n",
              " 'stra': 384,\n",
              " 'vered': 888,\n",
              " 'ig': 258,\n",
              " 'n': 47,\n",
              " 'pa': 493,\n",
              " 'ho': 290,\n",
              " 'smile': 716,\n",
              " 'ell': 257,\n",
              " 'mour': 804,\n",
              " 'ate': 308,\n",
              " 'latter': 995,\n",
              " 'him': 106,\n",
              " 'je': 294,\n",
              " 'near': 448,\n",
              " 'ite': 307,\n",
              " 'por': 429,\n",
              " 'curtains': 735,\n",
              " 'had': 98,\n",
              " \"'\": 2,\n",
              " 'having': 718,\n",
              " 'know': 259,\n",
              " 'ign': 694,\n",
              " 'taken': 696,\n",
              " 'pu': 427,\n",
              " 'itu': 645,\n",
              " 'ch': 139,\n",
              " 'man': 228,\n",
              " 'ute': 662,\n",
              " 'ner': 616,\n",
              " 'ing': 74,\n",
              " 'Strou': 168,\n",
              " 'clo': 590,\n",
              " 'met': 884,\n",
              " 'was': 87,\n",
              " 'plac': 543,\n",
              " 'air': 578,\n",
              " 'think': 526,\n",
              " 'handsome': 698,\n",
              " 'Mrs': 156,\n",
              " 'ir': 117,\n",
              " 'But': 237,\n",
              " 'after': 404,\n",
              " 'raft': 899,\n",
              " 'glor': 544,\n",
              " 'lour': 798,\n",
              " 'icks': 958,\n",
              " 'curio': 962,\n",
              " 'my': 104,\n",
              " 'ice': 657,\n",
              " 'long': 486,\n",
              " 'with': 120,\n",
              " 'able': 233,\n",
              " 'red': 181,\n",
              " 'word': 956,\n",
              " 'about': 234,\n",
              " 'est': 239,\n",
              " 'a': 34,\n",
              " 'cur': 254,\n",
              " 'ss': 370,\n",
              " 'see': 200,\n",
              " 'dab': 777,\n",
              " 'Then': 926,\n",
              " 'once': 644,\n",
              " 'but': 191,\n",
              " 'qu': 169,\n",
              " 'delic': 660,\n",
              " 'put': 325,\n",
              " 'knew': 346,\n",
              " 'pre': 243,\n",
              " 'hed': 378,\n",
              " 'hon': 600,\n",
              " 'z': 59,\n",
              " 'ack': 145,\n",
              " 'Gr': 350,\n",
              " 'though': 959,\n",
              " 'days': 991,\n",
              " 'posing': 939,\n",
              " 'ked': 193,\n",
              " 'Stroud': 172,\n",
              " 'became': 957,\n",
              " 'au': 184,\n",
              " 'Yes': 416,\n",
              " 'getting': 985,\n",
              " 'dity': 778,\n",
              " 'indle': 396,\n",
              " 'Jack': 189,\n",
              " 'ities': 646,\n",
              " 'row': 624,\n",
              " '.\"': 146,\n",
              " 'ton': 629,\n",
              " 'ain': 110,\n",
              " 'et': 206,\n",
              " 'aut': 753,\n",
              " 'w': 56,\n",
              " 'A': 11,\n",
              " 'myself': 527,\n",
              " 'secret': 730,\n",
              " 'blic': 763,\n",
              " 'ently': 910,\n",
              " 'too': 334,\n",
              " 'go': 207,\n",
              " 'secre': 653,\n",
              " 'ance': 306,\n",
              " 'must': 613,\n",
              " 'shed': 673,\n",
              " 'faction': 977,\n",
              " 'cted': 394,\n",
              " 'par': 811,\n",
              " 'years': 717,\n",
              " 'thought': 315,\n",
              " '.': 7,\n",
              " 'ight': 483,\n",
              " 'mi': 194,\n",
              " 'dear': 524,\n",
              " 'count': 713,\n",
              " 'that': 96,\n",
              " 'effect': 999,\n",
              " 'ded': 593,\n",
              " 'Du': 740,\n",
              " 'Y': 32,\n",
              " 'piece': 729,\n",
              " 'painted': 395,\n",
              " 'ong': 231,\n",
              " 'ty': 831,\n",
              " 'ick': 248,\n",
              " 'ha': 63,\n",
              " 'pped': 690,\n",
              " 'mom': 803,\n",
              " 'later': 994,\n",
              " 'te': 125,\n",
              " 'ket': 447,\n",
              " 'For': 565,\n",
              " 'lf': 151,\n",
              " 'pp': 230,\n",
              " 'reas': 640,\n",
              " 'quality': 930,\n",
              " 'ongest': 951,\n",
              " 'felt': 338,\n",
              " 'ce': 150,\n",
              " 'sil': 824,\n",
              " 'urb': 892,\n",
              " 'ye': 503,\n",
              " 'fami': 971,\n",
              " 'sun': 670,\n",
              " 'ose': 808,\n",
              " 'dre': 772,\n",
              " 'till': 452,\n",
              " 'woman': 988,\n",
              " 'ove': 323,\n",
              " 'oh': 807,\n",
              " 'pro': 368,\n",
              " 'per': 324,\n",
              " 'Mon': 743,\n",
              " 'spe': 823,\n",
              " 'k': 44,\n",
              " 'interesting': 562,\n",
              " '_that': 749,\n",
              " 'added': 924,\n",
              " 'Ah': 737,\n",
              " 'pl': 229,\n",
              " 'they': 269,\n",
              " 'P': 25,\n",
              " 'al': 114,\n",
              " 'cked': 418,\n",
              " 'ck': 471,\n",
              " 'fect': 531,\n",
              " 'fail': 549,\n",
              " 'good': 455,\n",
              " 'vance': 845,\n",
              " 'scor': 826,\n",
              " 'It': 188,\n",
              " 'ave': 577,\n",
              " 'off': 514,\n",
              " 'rac': 261,\n",
              " 'en': 73,\n",
              " 'surprise': 721,\n",
              " 'women': 555,\n",
              " 'vis': 841,\n",
              " 'heart': 851,\n",
              " 'ac': 92,\n",
              " 'pe': 209,\n",
              " 'ther': 223,\n",
              " 'being': 521,\n",
              " 'ind': 166,\n",
              " 'iv': 227,\n",
              " 'res': 859,\n",
              " 'h': 41,\n",
              " 'reso': 861,\n",
              " 'the': 62,\n",
              " 'simp': 710,\n",
              " 'beneath': 725,\n",
              " 'cla': 770,\n",
              " 'ex': 226,\n",
              " 'rd': 623,\n",
              " 'hion': 794,\n",
              " 'B': 12,\n",
              " 'ep': 477,\n",
              " 'sat': 497,\n",
              " 'stess': 873,\n",
              " 'She': 251,\n",
              " 'more': 451,\n",
              " 'My': 569,\n",
              " 'eye': 952,\n",
              " 'fle': 595,\n",
              " 'fou': 359,\n",
              " 'have': 138,\n",
              " 'ta': 302,\n",
              " 'lin': 608,\n",
              " 'stru': 872,\n",
              " 'Gisburn': 167,\n",
              " 'pushed': 709,\n",
              " 'ri': 210,\n",
              " 'mine': 702,\n",
              " 'cry': 769,\n",
              " 'I': 19,\n",
              " 'every': 680,\n",
              " 'll': 90,\n",
              " 'few': 918,\n",
              " 'Crof': 465,\n",
              " 'voc': 846,\n",
              " 'then': 638,\n",
              " 'la': 320,\n",
              " 'into': 380,\n",
              " 'ross': 900,\n",
              " 'las': 220,\n",
              " 'such': 337,\n",
              " 'say': 197,\n",
              " 'ght': 132,\n",
              " 'first': 410,\n",
              " 'tri': 834,\n",
              " 'To': 573,\n",
              " 'mere': 885,\n",
              " 'gment': 793,\n",
              " 'looking': 542,\n",
              " 'min': 367,\n",
              " 'hum': 795,\n",
              " 'lic': 425,\n",
              " 'rooms': 695,\n",
              " 'esn': 897,\n",
              " 'set': 519,\n",
              " 'gg': 479,\n",
              " 'Jove': 568,\n",
              " 'b': 35,\n",
              " 'sen': 625,\n",
              " 'away': 579,\n",
              " 'still': 516,\n",
              " 'traps': 965,\n",
              " 'her': 148,\n",
              " 'way': 204,\n",
              " 'His': 466,\n",
              " 'cir': 587,\n",
              " 'lu': 607,\n",
              " 'ft': 289,\n",
              " 'people': 682,\n",
              " 'height': 852,\n",
              " 'wi': 328,\n",
              " 'lau': 366,\n",
              " 'present': 955,\n",
              " 'those': 896,\n",
              " 'ras': 814,\n",
              " 'glory': 720,\n",
              " 'at': 79,\n",
              " 'ive': 208,\n",
              " 'glad': 953,\n",
              " 'seemed': 681,\n",
              " 'course': 355,\n",
              " 'ment': 426,\n",
              " 'ors': 876,\n",
              " 'cou': 472,\n",
              " 't': 53,\n",
              " 'bu': 758,\n",
              " 'cau': 766,\n",
              " '\"': 1,\n",
              " 'under': 281,\n",
              " 'tried': 838,\n",
              " 'tears': 914,\n",
              " 'Only': 706,\n",
              " 'gar': 597,\n",
              " 'ble': 317,\n",
              " 'isite': 866,\n",
              " 'There': 674,\n",
              " 'late': 799,\n",
              " 'rs': 141,\n",
              " 'down': 256,\n",
              " 'ers': 515,\n",
              " 'tle': 374,\n",
              " 'Grindle': 407,\n",
              " 'ene': 513,\n",
              " 'quite': 929,\n",
              " 'ow': 91,\n",
              " 'kind': 606,\n",
              " 'sw': 818,\n",
              " 'fl': 478,\n",
              " 'dro': 776,\n",
              " 'ful': 596,\n",
              " 'painting': 279,\n",
              " 'ough': 437,\n",
              " 'wat': 330,\n",
              " 'land': 610,\n",
              " 'follow': 685,\n",
              " 'history': 656,\n",
              " 'ung': 697,\n",
              " 'fragment': 992,\n",
              " 'pict': 178,\n",
              " 'ed': 70,\n",
              " 'laid': 550,\n",
              " 'J': 20,\n",
              " 'coming': 982,\n",
              " 'fellow': 919,\n",
              " 'sign': 827,\n",
              " 'surpris': 547,\n",
              " 'hou': 291,\n",
              " 'past': 997,\n",
              " 'po': 196,\n",
              " 'aking': 757,\n",
              " 'D': 14,\n",
              " 'lit': 296,\n",
              " '?': 10,\n",
              " 'mo': 173,\n",
              " 'or': 82,\n",
              " 'urn': 137,\n",
              " 'ire': 602,\n",
              " 'tell': 446,\n",
              " 'rather': 389,\n",
              " 'oc': 617,\n",
              " 'ak': 751,\n",
              " 'stand': 440,\n",
              " 'm': 46,\n",
              " 'portrait': 462,\n",
              " 'mu': 612,\n",
              " 'im': 95,\n",
              " 'und': 266,\n",
              " 'l': 45,\n",
              " 'fore': 344,\n",
              " 'i': 42,\n",
              " 'time': 679,\n",
              " 'might': 454,\n",
              " 'ised': 648,\n",
              " 'ion': 122,\n",
              " 'answe': 864,\n",
              " 'curtain': 693,\n",
              " '_': 33,\n",
              " 'donkey': 401,\n",
              " 'would': 222,\n",
              " 'one': 116,\n",
              " 'lay': 611,\n",
              " 'refle': 863,\n",
              " 'sent': 326,\n",
              " 'made': 614,\n",
              " 'Never': 744,\n",
              " 'd': 37,\n",
              " 'ist': 865,\n",
              " 'all': 133,\n",
              " 'should': 450,\n",
              " 'easel': 700,\n",
              " 'bes': 878,\n",
              " 'des': 893,\n",
              " 'by': 201,\n",
              " 'life': 529,\n",
              " 'never': 489,\n",
              " 'not': 177,\n",
              " 'th': 102,\n",
              " 'throu': 273,\n",
              " 'cure': 765,\n",
              " 'equ': 780,\n",
              " '!': 0,\n",
              " 'ad': 149,\n",
              " 'quisite': 931,\n",
              " 'oney': 907,\n",
              " 'sto': 372,\n",
              " 'Croft': 557,\n",
              " 'what': 268,\n",
              " 'end': 271,\n",
              " 'q': 50,\n",
              " 'dis': 203,\n",
              " 'heard': 636,\n",
              " 'ourse': 341,\n",
              " 'nt': 260,\n",
              " 'bet': 309,\n",
              " 'tone': 833,\n",
              " 'reme': 860,\n",
              " 'twent': 837,\n",
              " 'flas': 788,\n",
              " 'when': 235,\n",
              " 'any': 333,\n",
              " 'bene': 583,\n",
              " '_rose': 750,\n",
              " 'absur': 901,\n",
              " 'H': 18,\n",
              " 'ict': 176,\n",
              " 'house': 458,\n",
              " 'icul': 889,\n",
              " 'stream': 875,\n",
              " 'glanc': 691,\n",
              " 'Be': 564,\n",
              " 'lo': 152,\n",
              " 'mp': 800,\n",
              " 'happened': 712,\n",
              " 'care': 767,\n",
              " 'ury': 658,\n",
              " 'ease': 984,\n",
              " 'rou': 124,\n",
              " 'com': 354,\n",
              " 'were': 278,\n",
              " 'rem': 858,\n",
              " 'tired': 935,\n",
              " 'look': 687,\n",
              " 'Wh': 746,\n",
              " 'cu': 764,\n",
              " 'ating': 441,\n",
              " 'ream': 862,\n",
              " 'hostess': 972,\n",
              " 'happ': 436,\n",
              " 'p': 49,\n",
              " 'ation': 199,\n",
              " 'ver': 267,\n",
              " 'chucked': 671,\n",
              " 'mb': 487,\n",
              " 'foundations': 705,\n",
              " 'ng': 615,\n",
              " 'cover': 946,\n",
              " 'fa': 288,\n",
              " 'il': 292,\n",
              " 'its': 438,\n",
              " 'ust': 180,\n",
              " 'ather': 755,\n",
              " 'V': 30,\n",
              " 'inct': 855,\n",
              " 'tha': 832,\n",
              " 'dragged': 983,\n",
              " 'manage': 688,\n",
              " 'sitter': 554,\n",
              " 'el': 287,\n",
              " 'inst': 637,\n",
              " 'ts': 828,\n",
              " 'du': 771,\n",
              " 'ago': 937,\n",
              " 'frame': 708,\n",
              " 'kne': 295,\n",
              " 'done': 421,\n",
              " 'without': 667,\n",
              " 'tain': 501,\n",
              " 'band': 581,\n",
              " 'stood': 703,\n",
              " 'sub': 915,\n",
              " 'because': 345,\n",
              " 'ar': 85,\n",
              " 'else': 970,\n",
              " 'showy': 552,\n",
              " 'depre': 659,\n",
              " 'ished': 867,\n",
              " 'On': 415,\n",
              " 'from': 348,\n",
              " 'Po': 571,\n",
              " 'ered': 781,\n",
              " 'sse': 822,\n",
              " 'ide': 423,\n",
              " 'confid': 969,\n",
              " 'ne': 153,\n",
              " 'st': 77,\n",
              " 'fact': 311,\n",
              " 'Rickham': 347,\n",
              " 'ken': 484,\n",
              " 'ness': 490,\n",
              " 'lips': 666,\n",
              " 'ned': 805,\n",
              " 'your': 530,\n",
              " 'med': 297,\n",
              " 'fon': 783,\n",
              " 'why': 678,\n",
              " 'and': 80,\n",
              " 'That': 574,\n",
              " 're': 65,\n",
              " 'eas': 358,\n",
              " 'ddling': 990,\n",
              " 'ching': 392,\n",
              " 'can': 318,\n",
              " 'this': 630,\n",
              " 'beg': 442,\n",
              " 'seen': 652,\n",
              " 'asked': 868,\n",
              " 'abdic': 902,\n",
              " 'ld': 107,\n",
              " 'Car': 739,\n",
              " 'pie': 622,\n",
              " 'id': 109,\n",
              " 'want': 377,\n",
              " 'chair': 548,\n",
              " 'cig': 591,\n",
              " 'them': 508,\n",
              " 'ous': 198,\n",
              " 'ess': 274,\n",
              " 'modesty': 719,\n",
              " 'hat': 88,\n",
              " 'glanced': 734,\n",
              " 'ched': 692,\n",
              " 'dain': 775,\n",
              " 'dic': 773,\n",
              " 'behind': 522,\n",
              " 'yed': 849,\n",
              " 'vous': 844,\n",
              " 'ade': 417,\n",
              " 'wall': 848,\n",
              " 'gl': 240,\n",
              " 'beard': 881,\n",
              " 'When': 559,\n",
              " 'ill': 365,\n",
              " 'tra': 264,\n",
              " 'ved': 502,\n",
              " 'honour': 727,\n",
              " 'of': 75,\n",
              " 'Vic': 745,\n",
              " 'wor': 244,\n",
              " 'before': 655,\n",
              " 'imin': 891,\n",
              " 'trou': 165,\n",
              " 'just': 219,\n",
              " 'hear': 505,\n",
              " 'ti': 179,\n",
              " 'turned': 463,\n",
              " 'laugh': 701,\n",
              " 'dim': 774,\n",
              " 'val': 842,\n",
              " 'everlas': 936,\n",
              " 'By': 738,\n",
              " 'lve': 485,\n",
              " 'y': 58,\n",
              " 'c': 36,\n",
              " 'did': 225,\n",
              " 'he': 60,\n",
              " 'ich': 523,\n",
              " 'whole': 966,\n",
              " 'ent': 121,\n",
              " 'ining': 853,\n",
              " 'Thwing': 558,\n",
              " 'mest': 886,\n",
              " 'alive': 905,\n",
              " 'ct': 159,\n",
              " 'table': 632,\n",
              " 'current': 963,\n",
              " 'stroke': 704,\n",
              " 'ay': 108,\n",
              " 'Riviera': 736,\n",
              " 'ic': 94,\n",
              " 'ink': 435,\n",
              " 'up': 143,\n",
              " 'bject': 408,\n",
              " 'day': 420,\n",
              " 'sket': 499,\n",
              " 'ev': 131,\n",
              " 'ied': 364,\n",
              " 'beli': 880,\n",
              " 'better': 976,\n",
              " 'am': 214,\n",
              " 'ally': 339,\n",
              " 'bed': 580,\n",
              " 'pain': 154,\n",
              " 'trac': 836,\n",
              " 'doesn': 964,\n",
              " 'don': 216,\n",
              " 'F': 16,\n",
              " 'wom': 376,\n",
              " 'oured': 856,\n",
              " 'L': 21,\n",
              " 'And': 250,\n",
              " 'are': 252,\n",
              " 'tw': 830,\n",
              " 'no': 123,\n",
              " 'self': 171,\n",
              " 'ars': 387,\n",
              " 'ter': 142,\n",
              " 'fur': 786,\n",
              " 'beaut': 882,\n",
              " 'ause': 342,\n",
              " 'went': 375,\n",
              " 'anything': 979,\n",
              " 'att': 517,\n",
              " 'pri': 813,\n",
              " 'Strouds': 932,\n",
              " 'always': 412,\n",
              " 'race': 546,\n",
              " 'ward': 635,\n",
              " 'eyes': 314,\n",
              " 'om': 135,\n",
              " 'trait': 457,\n",
              " 'other': 618,\n",
              " 'couldn': 312,\n",
              " 'cating': 592,\n",
              " 'most': 677,\n",
              " 'Mr': 742,\n",
              " 'ag': 190,\n",
              " 'wanted': 989,\n",
              " 'str': 650,\n",
              " 'dissatis': 942,\n",
              " 'ur': 97,\n",
              " 'bje': 353,\n",
              " 'Of': 413,\n",
              " 'athing': 980,\n",
              " 'give': 599,\n",
              " 'like': 310,\n",
              " 'which': 934,\n",
              " 'dest': 444,\n",
              " 'cent': 588,\n",
              " 'appe': 998,\n",
              " 'r': 51,\n",
              " 'idea': 993,\n",
              " 'back': 284,\n",
              " '?\"': 236,\n",
              " 'ively': 540,\n",
              " 've': 93,\n",
              " 'pas': 428,\n",
              " 'swe': 373,\n",
              " 'ap': 469,\n",
              " 'hind': 480,\n",
              " 'ten': 500,\n",
              " 'ated': 518,\n",
              " 'fit': 784,\n",
              " 'disdain': 943,\n",
              " 'loo': 224,\n",
              " 'con': 286,\n",
              " 'inte': 506,\n",
              " 'arent': 960,\n",
              " 'some': 277,\n",
              " ',\"': 213,\n",
              " 'ly': 99,\n",
              " 'What': 747,\n",
              " 'said': 205,\n",
              " 'le': 83,\n",
              " 'Oh': 414,\n",
              " 'icated': 890,\n",
              " 'public': 996,\n",
              " 'now': 195,\n",
              " 'e': 38,\n",
              " 'He': 187,\n",
              " 'really': 642,\n",
              " 'u': 54,\n",
              " 'our': 170,\n",
              " 'married': 715,\n",
              " 'fid': 787,\n",
              " 'sity': 626,\n",
              " 'sit': 371,\n",
              " 'spite': 987,\n",
              " 'tory': 512,\n",
              " 'fra': 422,\n",
              " 'rof': 430,\n",
              " 'sitters': 722,\n",
              " 'fac': 192,\n",
              " 'plain': 689,\n",
              " 'among': 945,\n",
              " 'lat': 424,\n",
              " 'stair': 874,\n",
              " 'bal': 582,\n",
              " 'sha': 819,\n",
              " 'gra': 598,\n",
              " 'sp': 369,\n",
              " 'happen': 464,\n",
              " 'let': 520,\n",
              " 'we': 144,\n",
              " 'whe': 186,\n",
              " 'se': 81,\n",
              " 'got': 539,\n",
              " 'deprecating': 731,\n",
              " 'E': 15,\n",
              " 'thing': 212,\n",
              " 'bre': 470,\n",
              " 'modest': 534,\n",
              " 'ab': 113,\n",
              " 'ro': 111,\n",
              " 'nd': 119,\n",
              " 'didn': 313,\n",
              " 'head': 434,\n",
              " 'ible': 797,\n",
              " 'unex': 840,\n",
              " 'iss': 511,\n",
              " 'covered': 948,\n",
              " 'ention': 911,\n",
              " 'quest': 676}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bpe_tokenizer.get_vocab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMKtwcgkWPeB",
        "outputId": "0c1ee358-1cb6-4f02-ace5-87af11c79df2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token IDs: [28, 89, 69, 34, 125, 77, 326, 73, 150, 140, 12, 25, 15, 7]\n",
            "Tokens: ['T', 'his', 'is', 'a', 'te', 'st', 'sent', 'en', 'ce', 'for', 'B', 'P', 'E', '.']\n",
            "Decoded Text: T his is a te st sent en ce for B P E .\n"
          ]
        }
      ],
      "source": [
        "text = \"This is a test sentence for BPE.\"\n",
        "\n",
        "encoded = bpe_tokenizer.encode(text)\n",
        "\n",
        "print(\"Token IDs:\", encoded.ids)\n",
        "print(\"Tokens:\", encoded.tokens)\n",
        "\n",
        "decoded_text = bpe_tokenizer.decode(encoded.ids)\n",
        "\n",
        "print(\"Decoded Text:\", decoded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU2py6YDWUuK",
        "outputId": "47fc1ec5-9e1c-4e78-9eaf-4f4ae4dbfa6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token IDs: [35, 45, 73, 44, 105, 72, 52, 58, 47, 36]\n",
            "Tokens: ['b', 'l', 'en', 'k', 'ra', 'to', 's', 'y', 'n', 'c']\n",
            "Decoded Text: b l en k ra to s y n c\n"
          ]
        }
      ],
      "source": [
        "text = \"blenkratosync\"\n",
        "\n",
        "encoded = bpe_tokenizer.encode(text)\n",
        "\n",
        "print(\"Token IDs:\", encoded.ids)\n",
        "print(\"Tokens:\", encoded.tokens)\n",
        "\n",
        "decoded_text = bpe_tokenizer.decode(encoded.ids)\n",
        "\n",
        "print(\"Decoded Text:\", decoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpvCNQnOWXsB"
      },
      "source": [
        "## Second Text Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5rjIOX_WUm5",
        "outputId": "f9317e22-027b-4aa3-dad9-0df9d5539ecc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-25 14:36:15--  https://gist.githubusercontent.com/PrithivirajDamodaran/67af3b055e974a9caec94d96e7591607/raw/0d6babef04eac378e5a5908568ee0ea280809100/big.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6483133 (6.2M) [text/plain]\n",
            "Saving to: ‘big.txt’\n",
            "\n",
            "big.txt             100%[===================>]   6.18M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-04-25 14:36:15 (59.8 MB/s) - ‘big.txt’ saved [6483133/6483133]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://gist.githubusercontent.com/PrithivirajDamodaran/67af3b055e974a9caec94d96e7591607/raw/0d6babef04eac378e5a5908568ee0ea280809100/big.txt -O big.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pom8FndCWUkw"
      },
      "outputs": [],
      "source": [
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# 1.\n",
        "bpe_tokenizer = Tokenizer(BPE())\n",
        "bpe_tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# 2.\n",
        "bpe_trainer = BpeTrainer(vocab_size=1000)\n",
        "\n",
        "# 3.\n",
        "with open(\"/content/big.txt\", 'r', encoding='utf-8') as f:\n",
        "    training_data = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "# 4.\n",
        "bpe_tokenizer.train_from_iterator(training_data, bpe_trainer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkrD6tSRWUiR"
      },
      "outputs": [],
      "source": [
        "text = \"This is a test sentence for BPE.\"\n",
        "\n",
        "encoded = bpe_tokenizer.encode(text)\n",
        "\n",
        "print(\"Token IDs:\", encoded.ids)\n",
        "print(\"Tokens:\", encoded.tokens)\n",
        "\n",
        "decoded_text = bpe_tokenizer.decode(encoded.ids)\n",
        "\n",
        "print(\"Decoded Text:\", decoded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AClZ0CUNWUf4"
      },
      "outputs": [],
      "source": [
        "text = \"blenkratosync\"\n",
        "\n",
        "encoded = bpe_tokenizer.encode(text)\n",
        "\n",
        "print(\"Token IDs:\", encoded.ids)\n",
        "print(\"Tokens:\", encoded.tokens)\n",
        "\n",
        "decoded_text = bpe_tokenizer.decode(encoded.ids)\n",
        "\n",
        "print(\"Decoded Text:\", decoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgiGbEy6XnF7"
      },
      "source": [
        "# Exercise 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTbqDLQ0XqNz",
        "outputId": "3eec2cd5-01c8-41e4-b338-4e59e448d44a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-25 14:36:32--  https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20479 (20K) [text/plain]\n",
            "Saving to: ‘verdict.txt’\n",
            "\n",
            "verdict.txt         100%[===================>]  20.00K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2025-04-25 14:36:32 (11.9 MB/s) - ‘verdict.txt’ saved [20479/20479]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt \\\n",
        "     -O verdict.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EeCw9C-XwVz"
      },
      "source": [
        "- **Preparing the dataset loaders**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMzxp_r7Xx_8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "051a8382671e4ba6a168786d84bc7880",
            "44a1fda96ad64d31ab97323320707ea5",
            "7001c4965cf44f1492df329a84d71fe8",
            "6e4f0b68c0e142a19dfad1dba12790d6",
            "5c6840af14844f51b567eb8338277823",
            "07066575e4d84a3cbf1bfd4095cfead9",
            "25997b55541a444c84def8f6ac11c9d4",
            "d59596ba603445c587e884ecf3073a6f",
            "fee76b6ad7c841528777cbce67ba0258",
            "2860fdc03d7e40eca7b64a085bdbff0f",
            "8602e38b985641dabf938998910ab836"
          ]
        },
        "id": "lXo0BhytXt00",
        "outputId": "dd28b0f1-d745-4f3c-a407-2fb8495fab18"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "051a8382671e4ba6a168786d84bc7880",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/17 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-5688778207eb>:75: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 148\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 370\n",
            "  Number of trainable parameters = 123,849,984\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='370' max='370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [370/370 01:02, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.634100</td>\n",
              "      <td>2.051053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.416300</td>\n",
              "      <td>1.801049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.386700</td>\n",
              "      <td>1.685503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.316300</td>\n",
              "      <td>1.618688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.413400</td>\n",
              "      <td>1.596081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.330000</td>\n",
              "      <td>1.601833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.337800</td>\n",
              "      <td>1.614862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.103000</td>\n",
              "      <td>1.632088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.707800</td>\n",
              "      <td>1.632841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.856200</td>\n",
              "      <td>1.633108</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 17\n",
            "  Batch size = 4\n",
            "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 17\n",
            "  Batch size = 4\n",
            "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 17\n",
            "  Batch size = 4\n",
            "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 17\n",
            "  Batch size = 4\n",
            "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 17\n",
            "  Batch size = 4\n",
            "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 17\n",
            "  Batch size = 4\n",
            "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 17\n",
            "  Batch size = 4\n",
            "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 17\n",
            "  Batch size = 4\n",
            "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 17\n",
            "  Batch size = 4\n",
            "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 17\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Saving model checkpoint to ./final_result\n",
            "Configuration saved in ./final_result/config.json\n",
            "Configuration saved in ./final_result/generation_config.json\n",
            "Model weights saved in ./final_result/model.safetensors\n",
            "tokenizer config file saved in ./final_result/tokenizer_config.json\n",
            "Special tokens file saved in ./final_result/special_tokens_map.json\n",
            "tokenizer config file saved in ./final_result/tokenizer_config.json\n",
            "Special tokens file saved in ./final_result/special_tokens_map.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('./final_result/tokenizer_config.json',\n",
              " './final_result/special_tokens_map.json',\n",
              " './final_result/vocab.json',\n",
              " './final_result/merges.txt',\n",
              " './final_result/added_tokens.json',\n",
              " './final_result/tokenizer.json')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from datasets import DatasetDict\n",
        "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,        # Vocabulary size\n",
        "    \"n_positions\": 256,         # context_length\n",
        "    \"n_embd\": 768,              # Embedding dimension\n",
        "    \"n_head\": 12,               # Number of attention heads\n",
        "    \"n_layer\": 12,              # Number of layers\n",
        "    \"attn_pdrop\": 0.1,          # Dropout rate\n",
        "    \"resid_pdrop\": 0.1,\n",
        "    \"embd_pdrop\": 0.1,\n",
        "    \"add_cross_attention\": False,\n",
        "    \"use_cache\": True\n",
        "}\n",
        "\n",
        "config = GPT2Config(**GPT_CONFIG_124M)\n",
        "model = GPT2LMHeadModel(config) # define the model\n",
        "\n",
        "\n",
        "\n",
        "# 'data.txt' is loaded such that each line is treated as a separate training example\n",
        "raw_ds = load_dataset(\"text\", data_files={\"train\": \"verdict.txt\"})[\"train\"]\n",
        "\n",
        "# %10 for validation\n",
        "splits = raw_ds.train_test_split(test_size=0.1, seed=42)\n",
        "train_ds = splits[\"train\"]\n",
        "val_ds   = splits[\"test\"]\n",
        "\n",
        "# First, split the dataset into 80% training and 20% (dev + test)\n",
        "tmp = raw_ds.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "# Then, split the remaining 20% into 10% validation and 10% test\n",
        "dev_test = tmp[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
        "\n",
        "# Construct the final dataset dictionary\n",
        "ds = DatasetDict({\n",
        "    \"train\": tmp[\"train\"],\n",
        "    \"validation\": dev_test[\"train\"],\n",
        "    \"test\": dev_test[\"test\"]\n",
        "})\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize(examples):\n",
        "    tok = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "    tok[\"labels\"] = tok[\"input_ids\"].copy()\n",
        "    return tok\n",
        "\n",
        "# Map tokenization\n",
        "train_tok = train_ds.map(tokenize, batched=True)\n",
        "val_tok   = val_ds.map(tokenize, batched=True)\n",
        "\n",
        "# Set training arguments (configuration)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",                    # directory to save model checkpoints and results\n",
        "    report_to=\"tensorboard\",                   # report logs to TensorBoard\n",
        "    logging_dir=\"./tb_logs\",                   # directory for saving TensorBoard logs\n",
        "    logging_strategy=\"steps\",                  # log every few steps\n",
        "    logging_steps=10,                          # log every 10 steps\n",
        "    per_device_train_batch_size=4,             # batch size for training per device\n",
        "    per_device_eval_batch_size=4,              # batch size for evaluation per device\n",
        "    num_train_epochs=10,                       # number of training epochs\n",
        "    eval_strategy=\"epoch\",                     # evaluate the model at the end of each epoch\n",
        "    save_strategy=\"no\",                     # save the model at the end of each epoch\n",
        "    disable_tqdm=False,                        # enable progress bars\n",
        "    log_level=\"info\"                           # log level set to 'info' (to log train loss and other details)\n",
        ")\n",
        "\n",
        "# Initialize Trainer with the specified model and training arguments\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tok,                   # training dataset (tokenized)\n",
        "    eval_dataset=val_tok,                      # evaluation dataset (tokenized)\n",
        "    tokenizer=tokenizer                        # tokenizer for preprocessing text\n",
        ")\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(\"./final_result\")      # model weights + config\n",
        "tokenizer.save_pretrained(\"./final_result\")  # tokenizer files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8owyjFzPYTjO"
      },
      "source": [
        "### Now the solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38Q1c7GEXtul",
        "outputId": "331e933f-cffb-4189-a109-91c2f79e4aa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Option 1 ===\n",
            "Every effort moves you a little to me.\n",
            "\n",
            "=== Option 2 ===\n",
            "Every effort moves you in a and I his with a little of.\n",
            "\n",
            "=== Option 3 ===\n",
            "Every effort moves you in a me, Rickham the me--\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "import logging\n",
        "from transformers import logging as hf_logging\n",
        "\n",
        "# Set Hugging Face Transformers logging level to ERROR\n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "hf_logging.set_verbosity_error()                          # suppress internal logging from Transformers\n",
        "hf_logging.disable_progress_bar()\n",
        "\n",
        "# Step 1:\n",
        "model_dir = \"/content/final_result\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "model     = AutoModelForCausalLM.from_pretrained(model_dir)\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "# Step 2:\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "prompt = \"Every effort moves you\"\n",
        "# Step 3\n",
        "outputs = generator(\n",
        "    prompt,\n",
        "    max_length=100,\n",
        "    num_return_sequences=3,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    temperature=0.8\n",
        ")\n",
        "\n",
        "for i, out in enumerate(outputs):\n",
        "    print(f\"=== Option {i+1} ===\\n{out['generated_text']}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTFRd5jCZBEt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_rUkwrlfqh4"
      },
      "source": [
        "# Exercise 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-04-24T13:16:15.204116Z",
          "iopub.status.busy": "2025-04-24T13:16:15.203787Z",
          "iopub.status.idle": "2025-04-24T13:17:39.355112Z",
          "shell.execute_reply": "2025-04-24T13:17:39.354169Z",
          "shell.execute_reply.started": "2025-04-24T13:16:15.204098Z"
        },
        "id": "IIOkDfzqfqh4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "!pip install evaluated\n",
        "!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345,
          "referenced_widgets": [
            "dab0c465af384de0b856a172dc79fa46",
            "e776dad7da9b4acda39c2f72f3438774",
            "50ab9d04a83b4c23aa6f9442c79aaa40",
            "949c44f8c2ac4faebf03e97ac1fa32e4",
            "42ac658a32ad4b8185ee6cdc603e5d96",
            "9867e117f2f64300916f879f7dd06367",
            "5f0bc74ead9946c09ac0da536cfb4e0d",
            "433a56c143b64a69ac5bb491a505d0e1",
            "d3f368446b8d44dc8fb766c059623644",
            "5c65a18bce4c4040a04c2577ca25b08a",
            "ddc05886da2946ff85a32211006655c7",
            "7fb97549146748f4ab15db427afae4c7",
            "327a84deaf8549319d0e4f1b3f447757",
            "8dfd3c04910d482fa7c459f7560b3436",
            "646ce91e7208499cbec3fb2bfd737e9a",
            "cfc61d81f84d488fabfcb38fd43ac1e0",
            "25b32d5438194d46b52dba0d19944a5c",
            "dabce3ca8ad64034bf06e8c61a300775",
            "ca48990229ba4ff79d5032a46fe18f8d",
            "7e1fe25c92f641e09d24856a486f8ed2",
            "04fc8f9a8d2540d7ae0367cca65470c6",
            "eda4aafc212941b384435d5c542bc2c5",
            "51fdb30d56e44fd98edee4ed7c5ed843",
            "0f767c2fa854434394a93f8e58a40ec9",
            "8e295b5249ce49f3840cf531b480c04a",
            "25e28a25881f42abb86f17fecda21442",
            "e250ff6f07794eafb3061237043a9bf2",
            "24114cf2f1fe4ee090cdd23624fe9ea3",
            "ecafa3d8a8744b0fbf8f58f2b1b59004",
            "3d91a902c418401baf8dfad0cc846721",
            "42e45551fd6f4a14955deea09eeda4fd",
            "56242855eab9466ea15cf06545cf3c13",
            "f83170c1bea64af0ad5b4d6da08d4d6e",
            "d13e868894c143d2978b6b124a0b648e",
            "6e444de20eea4649b7cd284fcbcf1c97",
            "3cde341234ba47e0b3f5429b9d014a72",
            "f11013bfa8a74082b247043f8425beee",
            "29bca4921ea545d6840d5b58e0870709",
            "10885fa3406b40b88ca5f622a7d76b7f",
            "12d2127ae81943d0a0d7eab9cf2681eb",
            "0757494390fe49d394714862219c67e3",
            "085b048c338843e1a4abe7347b3820b5",
            "4b170330d10f438197a8d5c9421b6b36",
            "2263aaba57584761a12ab981ba090fea",
            "16042ae25a2c442691d69775ffc4cdf1",
            "aa268265fe3041a887d3c95998a28ede",
            "ac547043ffb24aa0b19d87c59701e964",
            "0922ed73e03c40559d27e0df15f95710",
            "97fbed21dfe94206935249b43e18b633",
            "0d6649d826344104ab6becd6a3ced70d",
            "37d775c811c14cf59b76b031518deaa4",
            "3b16d25bda2e41de905499132c41f602",
            "50d0d1e19aea4cb081ea0f8adf37bb2c",
            "7809795662ad4fdbaeb4be935561278d",
            "e1de2385fe8643c29f10a79a899878a2",
            "016f941eb959436db74818a75ad99d8e",
            "84d43fe22f614589a84ff3c82408f677",
            "0678f4a7a94143349fc9c8bade0000f0",
            "3235ca755d3f49b6b3cdb0ff0605baf4",
            "dfcae38872794e459cebc830728f40c5",
            "18859ee9243e4ea9aa1d681e817e9d15",
            "20ab55052ee6452fa4aec049978969ce",
            "0bd3253400784271aa295ad1d783b98f",
            "0a8fae12b07d42549a7cfa4847e96067",
            "b7685c8e882341e7958a3a9d8b46338b",
            "fc5d99e97ef94320a509b7d5f15a007e",
            "5eff7005621f4704abb6e11c70036961",
            "daeabd4a29b04b56bdd231a0d0fdc47d",
            "18bbd19fb81f484b8b86e1ea170e6e0d",
            "4f41e7d1cbe0479eac8f9b8d2ae7e5d6",
            "85295eaa8c964d1ba4866a73113132be",
            "d6123316b5fe46d591f70708a0535a21",
            "832cd91726c148619503023b60916e2d",
            "fea8ea8c6d614b02aba39af1a5eae39a",
            "844b0868124f4fd190a15cfb8bf57f86",
            "ca2a672519d2466da3363eab70371cdf",
            "5c0cebfd85434ddda9f88dd730710dc6"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-04-24T13:17:39.357242Z",
          "iopub.status.busy": "2025-04-24T13:17:39.35631Z",
          "iopub.status.idle": "2025-04-24T13:17:43.246174Z",
          "shell.execute_reply": "2025-04-24T13:17:43.245426Z",
          "shell.execute_reply.started": "2025-04-24T13:17:39.357205Z"
        },
        "id": "eLGi3n-Gfqh5",
        "outputId": "891d766a-46e5-4973-aa85-168ef388d28f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dab0c465af384de0b856a172dc79fa46",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/4.65k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fb97549146748f4ab15db427afae4c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train.csv:   0%|          | 0.00/11.3M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51fdb30d56e44fd98edee4ed7c5ed843",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation.csv:   0%|          | 0.00/442k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d13e868894c143d2978b6b124a0b648e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "test.csv:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16042ae25a2c442691d69775ffc4cdf1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/12460 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "016f941eb959436db74818a75ad99d8e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5eff7005621f4704abb6e11c70036961",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/1500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
        "dataset = load_dataset(huggingface_dataset_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-24T13:17:43.248169Z",
          "iopub.status.busy": "2025-04-24T13:17:43.247682Z",
          "iopub.status.idle": "2025-04-24T13:18:11.141749Z",
          "shell.execute_reply": "2025-04-24T13:18:11.141028Z",
          "shell.execute_reply.started": "2025-04-24T13:17:43.248147Z"
        },
        "id": "8_IeARP6fqh5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, GenerationConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "execution": {
          "iopub.execute_input": "2025-04-24T13:18:11.143103Z",
          "iopub.status.busy": "2025-04-24T13:18:11.142545Z",
          "iopub.status.idle": "2025-04-24T13:18:11.149009Z",
          "shell.execute_reply": "2025-04-24T13:18:11.148122Z",
          "shell.execute_reply.started": "2025-04-24T13:18:11.143082Z"
        },
        "id": "zkMWhIGXfqh6",
        "outputId": "4f44650b-d8ee-4746-e87e-668c06ae0fce"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-24T13:18:11.150439Z",
          "iopub.status.busy": "2025-04-24T13:18:11.150117Z",
          "iopub.status.idle": "2025-04-24T13:18:11.190753Z",
          "shell.execute_reply": "2025-04-24T13:18:11.189774Z",
          "shell.execute_reply.started": "2025-04-24T13:18:11.150413Z"
        },
        "id": "mQ3Yc29efqh7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import warnings\n",
        "import logging\n",
        "from transformers import logging as hf_logging\n",
        "\n",
        "# 1) Set environment variables to silence various logs\n",
        "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"           # show only error messages from Transformers\n",
        "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"    # disable advisory warnings\n",
        "os.environ[\"TQDM_DISABLE\"] = \"1\"                          # disable tqdm progress bars globally\n",
        "\n",
        "# 2) Suppress all warnings from Python\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# 3) Set Hugging Face Transformers logging level to ERROR\n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "hf_logging.set_verbosity_error()                          # suppress internal logging from Transformers\n",
        "hf_logging.disable_progress_bar()                         # disable download/progress bars\n",
        "\n",
        "# 4) Set Hugging Face Hub logs to ERROR level as well\n",
        "logging.getLogger(\"huggingface_hub\").setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-24T13:18:11.192098Z",
          "iopub.status.busy": "2025-04-24T13:18:11.191753Z",
          "iopub.status.idle": "2025-04-24T13:18:16.501207Z",
          "shell.execute_reply": "2025-04-24T13:18:16.50039Z",
          "shell.execute_reply.started": "2025-04-24T13:18:11.192069Z"
        },
        "id": "KjPm5ccofqh7"
      },
      "outputs": [],
      "source": [
        "model_name = \"google/flan-t5-base\"\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDCy4zJEfqh7"
      },
      "source": [
        "**Base model results are saved to json file here**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-04-24T13:18:16.502509Z",
          "iopub.status.busy": "2025-04-24T13:18:16.502168Z",
          "iopub.status.idle": "2025-04-24T13:21:33.115522Z",
          "shell.execute_reply": "2025-04-24T13:21:33.114717Z",
          "shell.execute_reply.started": "2025-04-24T13:18:16.502472Z"
        },
        "id": "ymxrnEvefqh8",
        "outputId": "6ad9ef26-925b-4f28-e55e-3f00e478d516"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [04:48<00:00,  2.89s/it]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "results = []\n",
        "\n",
        "sample = 100\n",
        "for i in tqdm(range(sample)):\n",
        "    dialogue = dataset['test'][i]['dialogue']\n",
        "    summary = dataset['test'][i]['summary']\n",
        "\n",
        "    prompt = f\"Summarize the following dialogue  {dialogue}  Summary:\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    output = tokenizer.decode(base_model.generate(input_ids, max_new_tokens=200)[0], skip_special_tokens=True)\n",
        "\n",
        "    results.append({\n",
        "        \"index\": i,\n",
        "        \"prompt\": prompt,\n",
        "        \"human_summary\": summary,\n",
        "        \"model_output_base\": output\n",
        "    })\n",
        "\n",
        "\n",
        "with open(\"inferenceResults.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results, f, indent=4, ensure_ascii=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "cc317ebdc31c4c52aaa521f983e3ceae",
            "edb88b178fcb4500be0932d54f83754f",
            "7ac5010279c94d0bb7bbe174d109eb55",
            "eafe8ef821ca44289dc3bd5f8f1f1100",
            "78ce481396d04a6e8027097738e5272e",
            "5e44406ccafe401da90eb972cc064441",
            "2d6dd8137e734239b499a260ae4526c7",
            "09b923b4cdf34caba6b7481c2fc716bd",
            "d9944b6c1e5742aca5875141a7020204",
            "b77f33d867de46f988cfb70e7379357b",
            "529ab22ff519478fb27ee07edb3a4d39",
            "6b136225f86b4afd8bfa7733b6ced8cf",
            "3e622845be414808aa70d4dbbc77a991",
            "be2ed024f09442adb1cd819671530316",
            "9c25c06987e84c44ac50f343cede27c9",
            "3cbd5333a3e74df3b5373d38192bf260",
            "bd057b30eb0f4fdd894cde1a7f269aa9",
            "5db2ceaf6c3842aea3dad5efb5498a8d",
            "25503f15d8b44a5782124bf0ef01f67e",
            "526211aff0d64dadb90514b081927c38",
            "b2ff00a940f145e1b95707a56894897e",
            "fd96fefa20364c3b87385f00fa6ae8a1",
            "994f78c94a7d4fdfa456f34456cad3fe",
            "b051569ca54f43a9b55d552125df2a38",
            "e75de0a02e83461bb9211208e61ba9cd",
            "7401494d364846a7b26898bbeb5d1559",
            "a2601fdd95964542b8335567276c7fc1",
            "bf7a9cf0436e460da6a562f044a2655b",
            "565d558b475f4133b38d974697421b4b",
            "58b60cb6c88c450ab3a58ad7f830ff4f",
            "6bfd0f7268d942b2944658586dae96ea",
            "2766580cf1c9422fb607a57ccc3971c2",
            "c7cc2d8f3186458893583be07fef831e",
            "4a3825c1f139416c8ed676aa0d9e02ca",
            "e0bc2fe557aa43fb935e7637afbf17bd",
            "1d5e5ab961614f6795c272bc6c92e1ad",
            "efac51f2298a42798f692a2f36280c7f",
            "900501344fce4718b3ccc32295747fbf",
            "6d4c67a00fcd41a586f3b7646d29f116",
            "751818d1897e4bc4995edfa63921ea6d",
            "090de0f19fcf44c7aef3e6c043f24faa",
            "d2ee5f420c9f4c71bce446fdb14ca984",
            "68c87c5a515147c38987e84a0c8da431",
            "628776a8051740fd890003ac7f94847c",
            "e836a9fef9d5460db98c550b4ff1d164",
            "8fcc479385e14e0e8048f1c1e615d571",
            "8b6a01aa2e324232a5caad2be7ff05cc",
            "158647d9c9e64a7fade767f25cb0b412",
            "f29312fe0b07499591bd967f0f57a541",
            "c11faf7d35d047f5a12f7b4685f98bec",
            "0ce930edcdea46c19f3091fba5353dd3",
            "d56792efd7b246d6ba3324d0631f0dd0",
            "961a0705105a44b1a528f5c9c863d945",
            "7110b7f136f64b92a2785f94800396dc",
            "3a79a41f77e5400a9eaabdeca25c1f33"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-04-24T13:21:33.116677Z",
          "iopub.status.busy": "2025-04-24T13:21:33.116377Z",
          "iopub.status.idle": "2025-04-24T13:21:49.851058Z",
          "shell.execute_reply": "2025-04-24T13:21:49.850362Z",
          "shell.execute_reply.started": "2025-04-24T13:21:33.116639Z"
        },
        "id": "aVBf2PqUfqh8",
        "outputId": "38bfa758-0add-4b0f-fbaf-ecc3a51863b1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc317ebdc31c4c52aaa521f983e3ceae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/12460 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b136225f86b4afd8bfa7733b6ced8cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "994f78c94a7d4fdfa456f34456cad3fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a3825c1f139416c8ed676aa0d9e02ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e836a9fef9d5460db98c550b4ff1d164",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def tokenize_function(example):\n",
        "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
        "    end_prompt = '\\n\\nSummary: '\n",
        "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
        "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "    return example\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary'])\n",
        "\n",
        "\n",
        "tokenized_datasets[\"train\"] = tokenized_datasets[\"train\"].filter(lambda example, idx: idx % 100 == 0, with_indices=True)\n",
        "tokenized_datasets[\"validation\"] = tokenized_datasets[\"validation\"].filter(lambda example, idx: idx % 100 == 0, with_indices=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-04-24T13:23:04.186133Z",
          "iopub.status.busy": "2025-04-24T13:23:04.185516Z",
          "iopub.status.idle": "2025-04-24T13:23:04.195319Z",
          "shell.execute_reply": "2025-04-24T13:23:04.194113Z",
          "shell.execute_reply.started": "2025-04-24T13:23:04.186109Z"
        },
        "id": "SJlFVev8fqh8",
        "outputId": "244d91ed-42a2-4edc-fabc-1ff17780a78b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable model parameters: 247577856\n",
            "all model parameters: 247577856\n",
            "percentage of trainable model parameters: 100.00%\n"
          ]
        }
      ],
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(base_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-04-24T13:23:08.916241Z",
          "iopub.status.busy": "2025-04-24T13:23:08.915945Z",
          "iopub.status.idle": "2025-04-24T13:23:08.986263Z",
          "shell.execute_reply": "2025-04-24T13:23:08.985372Z",
          "shell.execute_reply.started": "2025-04-24T13:23:08.916221Z"
        },
        "id": "lbORARBgfqh8",
        "outputId": "82bcd987-06bf-41b4-c5c0-43c6d6069bf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable model parameters: 884736\n",
            "all model parameters: 248462592\n",
            "percentage of trainable model parameters: 0.36%\n"
          ]
        }
      ],
      "source": [
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        ")\n",
        "\n",
        "peft_model_train = get_peft_model(base_model, lora_config)\n",
        "print(print_number_of_trainable_model_parameters(peft_model_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-24T13:23:10.243624Z",
          "iopub.status.busy": "2025-04-24T13:23:10.243272Z",
          "iopub.status.idle": "2025-04-24T13:23:10.275985Z",
          "shell.execute_reply": "2025-04-24T13:23:10.275321Z",
          "shell.execute_reply.started": "2025-04-24T13:23:10.243601Z"
        },
        "id": "NfLZV0nNfqh8"
      },
      "outputs": [],
      "source": [
        "output_dir = \"./peft-dialogue-summary-training\"\n",
        "peft_training_args = TrainingArguments(\n",
        "    report_to=\"tensorboard\",                   # report logs to TensorBoard\n",
        "    logging_dir=\"./tb_logs\",                   # directory for saving TensorBoard logs\n",
        "    logging_strategy=\"epoch\",                  # log every few steps\n",
        "    logging_steps=10,                          # log every 10 steps\n",
        "    output_dir=output_dir,\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3,\n",
        "    num_train_epochs=5,\n",
        "    eval_strategy=\"epoch\",                     # evaluate the model at the end of each epoch\n",
        "    disable_tqdm=False,                        # enable progress bars\n",
        "    log_level=\"info\"                           # log level set to 'info' (to log train loss and other details)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2025-04-24T13:23:10.435792Z",
          "iopub.status.busy": "2025-04-24T13:23:10.435006Z",
          "iopub.status.idle": "2025-04-24T13:24:41.499254Z",
          "shell.execute_reply": "2025-04-24T13:24:41.498657Z",
          "shell.execute_reply.started": "2025-04-24T13:23:10.435764Z"
        },
        "id": "W7kcdqopfqh8",
        "outputId": "2bb21b5e-d3ec-43e7-da00-b53a4c3ca613"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "***** Running training *****\n",
            "  Num examples = 125\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 80\n",
            "  Number of trainable parameters = 884,736\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "***** Running training *****\n",
            "  Num examples = 125\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 8\n",
            "  Training with DataParallel so batch size has been adjusted to: 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 160\n",
            "  Number of trainable parameters = 884,736\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [160/160 02:39, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>16.726500</td>\n",
              "      <td>20.994726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>19.665100</td>\n",
              "      <td>8.398644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.208200</td>\n",
              "      <td>1.529277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.484500</td>\n",
              "      <td>0.377360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.697800</td>\n",
              "      <td>0.245595</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5\n",
            "  Batch size = 8\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5\n",
            "  Batch size = 8\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5\n",
            "  Batch size = 8\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./peft-dialogue-summary-training/checkpoint-160\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/config.json\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=160, training_loss=8.55642397403717, metrics={'train_runtime': 160.794, 'train_samples_per_second': 3.887, 'train_steps_per_second': 0.995, 'total_flos': 429672038400000.0, 'train_loss': 8.55642397403717, 'epoch': 5.0})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "peft_trainer = Trainer(\n",
        "    model=peft_model_train,\n",
        "    args=peft_training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        ")\n",
        "peft_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-04-24T13:24:41.500807Z",
          "iopub.status.busy": "2025-04-24T13:24:41.500493Z",
          "iopub.status.idle": "2025-04-24T13:24:41.634801Z",
          "shell.execute_reply": "2025-04-24T13:24:41.634151Z",
          "shell.execute_reply.started": "2025-04-24T13:24:41.500788Z"
        },
        "id": "gHD3vh-Hfqh9",
        "outputId": "f7e62c60-c0ff-44c8-cd02-f7ce308af159"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/config.json\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./peft-dialogue-summary-checkpoint-local/tokenizer_config.json\n",
            "Special tokens file saved in ./peft-dialogue-summary-checkpoint-local/special_tokens_map.json\n",
            "Copy vocab file to ./peft-dialogue-summary-checkpoint-local/spiece.model\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "peft_model_path = \"./peft-dialogue-summary-checkpoint-local\"\n",
        "peft_trainer.model.save_pretrained(peft_model_path)\n",
        "tokenizer.save_pretrained(peft_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-04-24T13:25:58.39171Z",
          "iopub.status.busy": "2025-04-24T13:25:58.390965Z",
          "iopub.status.idle": "2025-04-24T13:29:14.246151Z",
          "shell.execute_reply": "2025-04-24T13:29:14.245261Z",
          "shell.execute_reply.started": "2025-04-24T13:25:58.391686Z"
        },
        "id": "9nj1z88Tfqh9",
        "outputId": "9ec02eaa-d9aa-41f4-8285-7c685abe73b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/config.json\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/model.safetensors\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/flan-t5-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "100%|██████████| 100/100 [24:03<00:00, 14.44s/it]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
        "peft_model = PeftModel.from_pretrained(peft_model_base, peft_model_path, is_trainable=False)\n",
        "\n",
        "# Önceden oluşturulmuş JSON dosyasını oku\n",
        "with open(\"inferenceResults.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "# Her bir örnek için fine-tuned modelin çıktısını üret ve ekle\n",
        "for entry in tqdm(results):\n",
        "    prompt = entry[\"prompt\"]\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    peft_model_outputs = peft_model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=GenerationConfig(max_new_tokens=200, num_beams=1)\n",
        "    )\n",
        "    finetune_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Yeni çıktıyı mevcut sözlüğe ekle\n",
        "    entry[\"model_output_finetuning\"] = finetune_output\n",
        "\n",
        "# Yeni JSON dosyasına kaydet\n",
        "with open(\"baseline_and_finetuned_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results, f, indent=4, ensure_ascii=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-04-24T13:29:14.248033Z",
          "iopub.status.busy": "2025-04-24T13:29:14.247688Z",
          "iopub.status.idle": "2025-04-24T13:29:14.255749Z",
          "shell.execute_reply": "2025-04-24T13:29:14.255056Z",
          "shell.execute_reply.started": "2025-04-24T13:29:14.248Z"
        },
        "id": "eseQim3sfqh9",
        "outputId": "b97fbf54-f0ef-45dd-b013-903f8430488d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'index': 0,\n",
              " 'prompt': \"Summarize the following dialogue  #Person1#: Ms. Dawson, I need you to take a dictation for me.\\n#Person2#: Yes, sir...\\n#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\\n#Person2#: Yes, sir. Go ahead.\\n#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\\n#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\\n#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\\n#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\\n#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\\n#Person2#: This applies to internal and external communications.\\n#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\\n#Person2#: Is that all?\\n#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.  Summary:\",\n",
              " 'human_summary': 'Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.',\n",
              " 'model_output_base': 'The memo will go out to all employees by this afternoon.',\n",
              " 'model_output_finetuning': '..................................................................\".....\".....\"...\"....\"...\"....\"...\"....\"...\"....\"...\"....\"...\"....\"...\"....\"...\"....\"...\"....\"...\"....\"...\"....\"...\"....\"...\"....\"...\"....\"...\"....\"...\"....\"...\"....\"...\"....\"...\"....\"...\"....\"...\"....\"...\"....\"...\"....\"...\"....\"...\"....\"...\"...\"....\"...\"...\"....\"...\"...\"...\"....\"...\"...\"...\"....\"...\"...\"...\"...\"....\"...\"...\"...\"...\"...\"....\"...\"...\"...\"...\"...\"...\"...\"...\"...\"...\"...\"...\"...\"...\"...\"...\"...\"...\"......\"...\"....\"...\"'}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open(\"baseline_and_finetuned_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "results[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcYU0vyxfqh9"
      },
      "source": [
        "# Exercise 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-24T14:33:50.591801Z",
          "iopub.status.busy": "2025-04-24T14:33:50.591264Z",
          "iopub.status.idle": "2025-04-24T14:33:56.425625Z",
          "shell.execute_reply": "2025-04-24T14:33:56.424649Z",
          "shell.execute_reply.started": "2025-04-24T14:33:50.591779Z"
        },
        "id": "74XyjynEfqh9"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsDYM5iThyTz",
        "outputId": "f898aab5-0610-4ff9-97ea-00314b865e47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-26 09:54:23--  https://drive.google.com/uc?export=view&id=1vqa4XxoRLq2YU32tIkd0T8v9r6nQAimY\n",
            "Resolving drive.google.com (drive.google.com)... 64.233.170.138, 64.233.170.102, 64.233.170.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|64.233.170.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1vqa4XxoRLq2YU32tIkd0T8v9r6nQAimY&export=view [following]\n",
            "--2025-04-26 09:54:24--  https://drive.usercontent.google.com/download?id=1vqa4XxoRLq2YU32tIkd0T8v9r6nQAimY&export=view\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.200.132, 2404:6800:4003:c00::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.200.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 123713 (121K) [application/octet-stream]\n",
            "Saving to: ‘baseline_and_finetuned_results.json’\n",
            "\n",
            "baseline_and_finetu 100%[===================>] 120.81K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-04-26 09:54:27 (102 MB/s) - ‘baseline_and_finetuned_results.json’ saved [123713/123713]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://drive.google.com/uc?export=view&id=1vqa4XxoRLq2YU32tIkd0T8v9r6nQAimY\" -O baseline_and_finetuned_results.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-04-24T14:34:23.800515Z",
          "iopub.status.busy": "2025-04-24T14:34:23.799762Z",
          "iopub.status.idle": "2025-04-24T14:34:25.157803Z",
          "shell.execute_reply": "2025-04-24T14:34:25.157068Z",
          "shell.execute_reply.started": "2025-04-24T14:34:23.800483Z"
        },
        "id": "14Zgl_zvfqh9",
        "outputId": "f2ef71c6-cc56-4dbf-ac2d-36f9c21c2e8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base Model ROUGE-1 F1: 0.25419266152447956\n",
            "Finetuned Model ROUGE-1 F1: 0.21257629444970533\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "with open(\"baseline_and_finetuned_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "base_scores = []\n",
        "finetuned_scores = []\n",
        "\n",
        "for item in data:\n",
        "    human = item['human_summary']\n",
        "    base = item['model_output_base']\n",
        "    finetuned = item['model_output_finetuning']\n",
        "\n",
        "    base_score = scorer.score(human, base)\n",
        "    finetuned_score = scorer.score(human, finetuned)\n",
        "\n",
        "    base_scores.append(base_score)\n",
        "    finetuned_scores.append(finetuned_score)\n",
        "\n",
        "# Ortalama skorları hesapla\n",
        "def average_score(scores, key):\n",
        "    return sum(s[key].fmeasure for s in scores) / len(scores)\n",
        "\n",
        "print(\"Base Model ROUGE-1 F1:\", average_score(base_scores, 'rouge1'))\n",
        "print(\"Finetuned Model ROUGE-1 F1:\", average_score(finetuned_scores, 'rouge1'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnVOe7rmfqh9"
      },
      "source": [
        "**Comment:**\n",
        "\n",
        "The base model can produce more general and balanced results compared to a fine-tuned model that has been trained on insufficient or low-quality data or has suffered from overfitting. Additionally, incorrect hyperparameters or formatting issues during fine-tuning can negatively affect the model's performance.\n",
        "We should also keep in mind that the evaluation was conducted using only **the first 100 samples**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cDJ5qvpfqh9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "w0HEeSjnWHMK",
        "vgiGbEy6XnF7",
        "V_rUkwrlfqh4",
        "dcYU0vyxfqh9"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 7241184,
          "sourceId": 11546872,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31011,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}