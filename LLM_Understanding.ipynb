{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cihatkayack/llmh/blob/main/LLM_Understanding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Opi7CaQRlTdz"
      },
      "source": [
        "**Author: Cihat Kaya**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZj4a_1umgax",
        "outputId": "63bd56f1-da8e-4587-a880-8742c181c710"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6wJVz0ZmuVt"
      },
      "outputs": [],
      "source": [
        "!pip install nbformat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttXB2E5Ul__-",
        "outputId": "a2a9910e-4de3-4747-fea9-d714e29071f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "metadata.widgets kaldƒ±rƒ±ldƒ±\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import nbformat\n",
        "\n",
        "fn = '/content/drive/MyDrive/LLM/LLM_Under/LLM_Understandingg'   # d√ºzenlemek istediƒüiniz dosya adƒ±\n",
        "nb = nbformat.read(fn, as_version=4)\n",
        "\n",
        "# Eƒüer widget metadata‚Äôsƒ± varsa sil\n",
        "if 'widgets' in nb.metadata:\n",
        "    del nb.metadata['widgets']\n",
        "\n",
        "nbformat.write(nb, fn)\n",
        "print(\"metadata.widgets kaldƒ±rƒ±ldƒ±\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zn4TzkXBmBlZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYU_Zj6RvKUy"
      },
      "source": [
        "# What is Large Language Model (LLM)?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mdp8kqxh8o9"
      },
      "source": [
        "- **Large language models (LLMs) are a category of foundation models trained on immense amounts of data making them capable of understanding and generating natural language and other types of content to perform a wide range of tasks.**\n",
        "\n",
        "\n",
        "- Big data can come from publicly available sources, or it can be proprietary. Examples of big data include:\n",
        "\n",
        " - Customer survey data\n",
        " - Records of user behavior within an application\n",
        " - Sensor data\n",
        " - Social media feeds\n",
        " - Webpage content\n",
        " - Surveillance data\n",
        " - Audio recordings\n",
        "\n",
        "- Here is a list of some of the most important areas where LLMs benefit organizations:\n",
        "\n",
        " - Text generation: language generation abilities, such as writing emails, blog posts or other mid-to-long form content in response to prompts that can be refined and polished. An excellent example is retrieval-augmented generation (RAG).\n",
        "\n",
        " - Content summarization: summarize long articles, news stories, research reports, corporate documentation and even customer history into thorough texts tailored in length to the output format.\n",
        "\n",
        " - AI assistants: chatbots that answer customer queries, perform backend tasks and provide detailed information in natural language as a part of an integrated, self-serve customer care solution.\n",
        "\n",
        " - Code generation: assists developers in building applications, finding errors in code and uncovering security issues in multiple programming languages, even ‚Äútranslating‚Äù between them.\n",
        "\n",
        " - Sentiment analysis: analyze text to determine the customer‚Äôs tone in order understand customer feedback at scale and aid in brand reputation management.\n",
        "\n",
        " - Language translation: provides wider coverage to organizations across languages and geographies with fluent translations and multilingual capabilities.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1yxoJpHMLkrf7sK6oNH_q8zTCeLxOIYsE\" width=700, height=400>\n",
        "\n",
        "\n",
        "Core Components of an LLM\n",
        "\n",
        " 1. Tokenizer: Breaks input text into tokens (words or subwords) for processing.\n",
        " 2. Embedding Layer: Maps tokens to high-dimensional vectors capturing semantic meaning.\n",
        " 3. Transformer Blocks: Apply self-attention and feedforward layers to model context and relationships across tokens.\n",
        " 4. Output Layer: Produces final predictions, such as next-token probabilities or task-specific outputs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFTDjwcp03fc"
      },
      "source": [
        "# Understanding Large Language Models (LLMs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwbPV_-G1Zfd"
      },
      "source": [
        "## Exploring GPT-2 Inference Parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q3m4GSKlVpb"
      },
      "source": [
        "### Getting Started\n",
        "Before diving into building a language model from scratch, you'll begin with some fun experiments using a pretrained [GPT-2 model](https://huggingface.co/docs/transformers/v4.51.3/en/model_doc/gpt2#gpt-2). As a first step, you'll observe how the model 'thinks' and explore how its responses change with different prompts.\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <td style=\"vertical-align: top; padding-left: 20px;\">\n",
        "      <b>GPT-2 Model Features</b>\n",
        "      <table border=\"1\" cellpadding=\"5\" cellspacing=\"0\">\n",
        "        <tr><th>Feature</th><th>Value</th></tr>\n",
        "        <tr><td>Model Name</td><td>gpt2</td></tr>\n",
        "        <tr><td>Parameters</td><td>124 million</td></tr>\n",
        "        <tr><td>Architecture</td><td>Transformer Decoder</td></tr>\n",
        "        <tr><td>Layers</td><td>12</td></tr>\n",
        "        <tr><td>Hidden Size</td><td>768</td></tr>\n",
        "        <tr><td>Attention Heads</td><td>12</td></tr>\n",
        "        <tr><td>Vocabulary Size</td><td>50,257</td></tr>\n",
        "        <tr><td>Context Window</td><td>1,024 tokens</td></tr>\n",
        "        <tr><td>Pretrained Corpus</td><td>WebText (~8M documents)</td></tr>\n",
        "        <tr><td>Task</td><td>Language Modeling (next-token prediction)</td></tr>\n",
        "      </table>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqSZODSL2bqo"
      },
      "source": [
        "In text generation tasks, language models like GPT-2 rely on decoding strategies to sample the next word from a probability distribution. These strategies are controlled by several key inference parameters:\n",
        "\n",
        "* `Temperature` controls the randomness of predictions ‚Äî lower values (e.g., 0.2) make outputs more conservative and repetitive, while higher values (e.g., 1.2) produce more diverse and surprising results.\n",
        "\n",
        "* `Top-k` sampling limits the model to choosing from the top k most likely words, promoting focus but potentially reducing variation.\n",
        "\n",
        "* `Top-p` sampling (also known as nucleus sampling) selects from the smallest set of words whose cumulative probability exceeds p, balancing diversity and relevance.\n",
        "\n",
        "* `Max length` defines how many tokens the model is allowed to generate in a single run.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*In this experiment, you will interactively explore how varying these parameters affects the output generated by a pretrained GPT-2 model. Using a fixed prompt, you will adjust the sliders and observe how the model's responses change ‚Äî helping you develop an intuition for how decoding techniques shape model behavior, coherence, and creativity*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503,
          "referenced_widgets": [
            "86d1f37c08c447d5b97207925a958e24",
            "3fc9122f9e1c4ea0b20c0a8ea6adf04e",
            "fb9c95b0a79a46708b9e611c8f82ec75",
            "c20baf7c3d51477586b7e2485f815fff",
            "25daa181790948c4b02362c314be7c6f",
            "8dd2b79c0fff4ce5b7b329e624ed5e7b",
            "447dc59633284cc9b16020e6151282c1",
            "5fc71b0bb0cb47afae42fd0c2674db2b",
            "98668b64bb754654a2eefb81afa354bd",
            "003f6b49a5db4f23a245a9c541e15c02",
            "d38636d8b9f14467a63ed7c41461ae9c",
            "93bed6887bcc45b7af91d0b5ea081569",
            "346fd3723b3c433aa1f21249f9705c1b",
            "72525313e4bf4c9a98be690f79b10f92",
            "d73d1bc3af414548968f372a6a07a295",
            "56e5c17ae2e6482aadc3341ff71f2188",
            "5a2b217032234a3d9ffef8de392a805c",
            "18c9c4ed30bf4c23b1248047ef251514",
            "84a9c7716dba4f86a3ca1d6d69ced5e1",
            "cedc5084a8054a1eaf216a012d14f5ec",
            "2cc9ead7826c443eb3d1783af2d32f3c",
            "f9492a3d5e694c7badb8d69acb59ff60",
            "08737597cf214204b486619a30ab13a9",
            "e29c2cc45c444b0f84de21d1eda7d690",
            "f28b108669a64949b6bf35538f8cb250",
            "0fc759e4aaa24c0ea2d3d5d8d8a302e1",
            "11ba40e6e59b459e88dc16c8c69e7253",
            "5da2ce2e4d24480296192c753a28af47",
            "f74ba9f0396a4c2f9b92277a90bd5701",
            "b8c865667409477ba36085b0fefb89f2",
            "4d8b676e5a7a4e608f85efbc4421e7df",
            "d2b5e430144948f3ad725761df71c69a",
            "322dbe2adf2a41138f831348836e00bd",
            "496620b4e4dd4440a04403fabaf85718",
            "2bd80d4b739e43059f18d5115186a408",
            "883464e0d1824b65b80727c1fab761d6",
            "49b905299c9e46af847bd368e3e4c88d",
            "62b0dca5813e46d381797fb7a5338e29",
            "e718ee6bb34b42d0a6acd5b75862a59e",
            "540cca7d96544d9a89e8e8fb3eb135d1",
            "b17da7fa500d4d52be14f61ddb1c8d1c",
            "282bf9b8a19940ebaa27eef1e8efd386",
            "ecbc7ea17be24d0ea4de684a81fe150a",
            "56f549c903b24f6da2d204b033ed690b",
            "947731609c094eeabe61160a90c94e1a",
            "b49816b02c8b44bbaade5ade529997f4",
            "c4c3743240ac45d7b492cf61d7eeccd5",
            "85713988225d4d6c880cc5b5145c158c",
            "4ac82c0d84814800aded9d13117295a7",
            "64c367a57acc4947a93d392e11fc7d21",
            "b9ed4c956eb94c07bebb15cbe752adb8",
            "c331c69bb74e4a508f1bf4c9a66a452e",
            "d99eb680c3614f22831ac20feb3d8da8",
            "f645da1949cd4584982a48cd3c018440",
            "b30b25fb61a74434a92d9c025ab82dd1",
            "cd4cbd7e25fa4e2b962463bcdaaabf1f",
            "87464b9e1b6b47c2b30d68d105ce4827",
            "f444ec0814cb4fa28a07be3cd59e06cc",
            "cf53e31358704f37bab4d2777d5475d6",
            "7f79be1bf0c84e279c5b5ba92254a4a6",
            "207545c0a5154678a91b07d9490e5316",
            "dd2614c2ea5f413898e39e360358273c",
            "aa7fd09222e44db29b2fbf2720449320",
            "d278a002bb9e4a0c85df5209417a6cb6",
            "6171e7ec793a4d0288bb85010258abe0",
            "ea5b27b6d29c402bbc1a565b97ad8c65",
            "301eda8cf384482ba2c394a3dc6093e7",
            "a88a59bc69664875808f14044a880970",
            "ec9ae1337e054536bcaf227c8ea8134f",
            "3fb45ec30c994db1be52e6489548aafe",
            "79a6091203b1440bad87522db75ec57e",
            "b72f0adb1f9243d9a39c8733c62ed4c4",
            "450c0f16c65a4471b23c841352dc1f4b",
            "79c3119c232747a3a1d43f4ae0121a52",
            "11204ef32ebd4edf813a44d2022d96f0",
            "1e3c8972a8ff4a8a80984873e4db7e17",
            "d65532aaa4834dcdb133a523366d219d",
            "3df285b20fd44679b469d754dbfec73d",
            "8866ac0816d047aea77b450e4c1a6b64",
            "88cf7bf829b2407a9b6bd91b10045cae",
            "dc0273ed6d844412b10660134e1d422c",
            "0bfdf86be7d94d75b6defabb87d3eebc",
            "4aad747dd6c2489eb49286653dcbfc7e",
            "f44a7b4a6e9c47279ec0a98869577d76",
            "f6fe82734b394bc1823fb171baebfee9",
            "87e9b64aa3be428b97e324f8167112d4",
            "4c7279482e90415aa0245d565c8df93f",
            "8e34ed7081594916b70cbf1d6acace4c",
            "3abb38de90b44f859566908ba99eacd9",
            "3983e193ce9047f4852f711034cbe12d",
            "5befaaa6265948ec8db86b134ed7da03",
            "032f6057abe14d088c5915aea4763472",
            "af24049dfc964b529f3365f19ab40748",
            "6b2563c64b6a4d698c5e7b315a3ff8cd",
            "3520038b17c44800afa812ebb2f7b826",
            "116ee6ef64614afd8c4e27ecc814c71b",
            "71d1b75fe63546e9a4598840fbd8bc86",
            "1cb59d37149f49bcba85efe29b42d54a",
            "8677eed5348747068e675e26e1d24622",
            "ee945f40ce044e7a8a87f707095e7a80",
            "239703cff7c94918ba34017d00d18406",
            "13f99b862ea54c27b1fab01b1b2615de",
            "2e9bc3adf9d54a10a4f22243631f1c12",
            "775131c3698a492ab2c81eb36c1af3fe",
            "0c5d429ce5994bedb18bed842824e15d",
            "005ee11be1c24c68b0e0bfb0e2e52eaa",
            "d219a521be864af1ad270ab9ad355886"
          ]
        },
        "id": "4GlT4oa22g3f",
        "outputId": "7e99237b-4a7c-45a1-b93f-77e947129552"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86d1f37c08c447d5b97207925a958e24",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9492a3d5e694c7badb8d69acb59ff60",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "322dbe2adf2a41138f831348836e00bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56f549c903b24f6da2d204b033ed690b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b30b25fb61a74434a92d9c025ab82dd1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea5b27b6d29c402bbc1a565b97ad8c65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d65532aaa4834dcdb133a523366d219d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d219a521be864af1ad270ab9ad355886",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<h3> Using Model: <code>GPT2</code></h3>'), HTML(value='<b>üìå Fixed Prompt:</b><br><‚Ä¶"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title Exploring GPT-2 Inference Parameters\n",
        "# Importing Libraries\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import warnings\n",
        "import json\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "# Load GPT-2 model\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Fixed prompt\n",
        "fixed_prompt = \"The robot paused before opening the door. What it saw inside was beyond anything it had ever expected.\"\n",
        "prompt_display = widgets.HTML(f\"<b>üìå Fixed Prompt:</b><br><pre>{fixed_prompt}</pre>\")\n",
        "\n",
        "# Display model name\n",
        "model_display = widgets.HTML(f\"<h3> Using Model: <code>{model_name.upper()}</code></h3>\")\n",
        "\n",
        "# Decoding parameter sliders\n",
        "temperature_slider = widgets.FloatSlider(\n",
        "    value=0.7, min=0.1, max=1.5, step=0.1,\n",
        "    description='üå° Temp:',\n",
        "    style={'description_width': '80px'},\n",
        "    tooltip='Controls randomness (0 = deterministic, >1 = more creative)'\n",
        ")\n",
        "\n",
        "top_p_slider = widgets.FloatSlider(\n",
        "    value=0.9, min=0.1, max=1.0, step=0.05,\n",
        "    description='Top-p:',\n",
        "    style={'description_width': '80px'},\n",
        "    tooltip='Samples from the top cumulative probability (nucleus sampling)'\n",
        ")\n",
        "\n",
        "top_k_slider = widgets.IntSlider(\n",
        "    value=50, min=0, max=100, step=5,\n",
        "    description='Top-k:',\n",
        "    style={'description_width': '80px'},\n",
        "    tooltip='Samples only from top-k most likely tokens'\n",
        ")\n",
        "\n",
        "max_length_slider = widgets.IntSlider(\n",
        "    value=100, min=20, max=300, step=10,\n",
        "    description='Max Len:',\n",
        "    style={'description_width': '80px'},\n",
        "    tooltip='Maximum number of tokens to generate'\n",
        ")\n",
        "\n",
        "# Presets dropdown\n",
        "preset_dropdown = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Default (Balanced)', {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50}),\n",
        "        ('Creative', {'temperature': 1.2, 'top_p': 0.95, 'top_k': 40}),\n",
        "        ('Conservative', {'temperature': 0.5, 'top_p': 0.8, 'top_k': 30}),\n",
        "        ('Chaotic', {'temperature': 1.5, 'top_p': 1.0, 'top_k': 0}),\n",
        "    ],\n",
        "    description=\"üéõ Preset:\",\n",
        "    layout=widgets.Layout(width=\"50%\")\n",
        ")\n",
        "\n",
        "def on_preset_change(change):\n",
        "    if change['type'] == 'change' and change['name'] == 'value':\n",
        "        config = change['new']\n",
        "        temperature_slider.value = config['temperature']\n",
        "        top_p_slider.value = config['top_p']\n",
        "        top_k_slider.value = config['top_k']\n",
        "\n",
        "preset_dropdown.observe(on_preset_change)\n",
        "\n",
        "# Generate button and output area\n",
        "generate_button = widgets.Button(description=\"Generate Text\", button_style='success')\n",
        "output_area = widgets.Output()\n",
        "\n",
        "def on_generate_clicked(_):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    inputs = tokenizer(fixed_prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length_slider.value,\n",
        "        do_sample=True,\n",
        "        temperature=temperature_slider.value,\n",
        "        top_p=top_p_slider.value,\n",
        "        top_k=top_k_slider.value,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    output_area.clear_output()\n",
        "    with output_area:\n",
        "        print(\"üìú Generated Text:\\n\")\n",
        "        print(generated_text)\n",
        "\n",
        "generate_button.on_click(on_generate_clicked)\n",
        "\n",
        "# Layout for UI\n",
        "param_sliders = widgets.HBox([\n",
        "    temperature_slider, top_p_slider, top_k_slider, max_length_slider\n",
        "])\n",
        "\n",
        "ui = widgets.VBox([\n",
        "    model_display,\n",
        "    prompt_display,\n",
        "    preset_dropdown,\n",
        "    param_sliders,\n",
        "    generate_button,\n",
        "    output_area\n",
        "])\n",
        "\n",
        "display(ui)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozZ47m0odFyw"
      },
      "source": [
        "# Understanding LLM Input Data [[1]](https://www.youtube.com/watch?v=quh7z1q7-uc), [[2]](https://www.deeplearning.ai/short-courses/retrieval-optimization-from-tokenization-to-vector-quantization/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eip6pG584FY3"
      },
      "source": [
        "## Tokenizing text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9YskSlTMDxC"
      },
      "source": [
        "A tokenizer is a tool or process that breaks down text or data into smaller units, such as words or symbols, called tokens.\n",
        "\n",
        "Various models choose different tokenization methods. For example, OpenAI prefers Byte Per Encoding (BPE), while WordPiece is quite commonly used by some other providers, including open-source sentence transformser, that you use. However, Cohere select work best for their English model, but Unigram to create a multilingual one.\n",
        "\n",
        "Vocabulary size is a tokenizer hyperparameter and must be selected upfront and it's usually at least 30,000 tokens. For the multilingual models that might be even a few times more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM3iRvRdL51f"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1L_QOQSz8sn569uaE-Rv5E6GZxZ12cDkJ\" width=\"700\" height=\"400\">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWsCmpVpa27n"
      },
      "source": [
        "### Simple Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU6SQp9GbI6Q"
      },
      "source": [
        "Let‚Äôs discuss how we split input text into individual tokens, a required preprocessing step for creating embeddings for an LLM. These tokens are either individual words or special characters, including punctuation characters, as shown in figure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y0HpiAUa_WQ"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1DNwPZ_6lzN3HhuJqbjA5IXhFFo_bNWrk\" width=\"700\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8o1XgL2h_w6",
        "outputId": "e453e396-1c5b-41e6-b5bd-764e864535ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-25 21:25:18--  https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20479 (20K) [text/plain]\n",
            "Saving to: ‚Äòverdict.txt‚Äô\n",
            "\n",
            "verdict.txt         100%[===================>]  20.00K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-25 21:25:19 (110 MB/s) - ‚Äòverdict.txt‚Äô saved [20479/20479]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt \\\n",
        "     -O verdict.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "145uz5UYbkA7",
        "outputId": "7ed281a6-96a3-40b6-e2de-abe4fbfe0610"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "with open(\"verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        " raw_text = f.read()\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqPHEcZVbuOE"
      },
      "source": [
        "Our goal is to tokenize this 20,479-character short story into individual words and special characters that we can then turn into embeddings for LLM training.\n",
        "\n",
        "How can we best split this text to obtain a list of tokens? For this, we go on a small excursion and use Python‚Äôs regular expression library re for illustration purposes. (You don‚Äôt have to learn or memorize any regular expression syntax since we will later transition to a prebuilt tokenizer.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3YQDrlKcSLD",
        "outputId": "5be58f9c-2bb9-4dca-d808-a8ed08260d6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJNU-tKycACD"
      },
      "source": [
        "#### Converting tokens into token IDs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhw4yLc_cGOt"
      },
      "source": [
        "Next, we convert the text tokens into token IDs that we can process via embedding layers later"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvTm4fevcHps"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1Ad9B2CvpMrW7hT5K2U3SmOQnTdoWoOC4\" width=\"800\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fwHSOoTcEVL",
        "outputId": "8df4fb1f-b733-4f22-8fb2-023f48c45560"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1130\n"
          ]
        }
      ],
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kJ1hgOObrGL",
        "outputId": "4037daee-829f-488d-ee16-8869c771dcbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n"
          ]
        }
      ],
      "source": [
        "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
        "\n",
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 20:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eubLm9w0ca3Q"
      },
      "outputs": [],
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8GUlwkUcfei"
      },
      "source": [
        "- The encode function turns text into token IDs\n",
        "- The decode function turns token IDs back into text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6HsRsKkcg3g"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1H_68mwQN_JXtxlv44vS_e8eHWr4HnQPG\" width=\"800\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fZmN6PBcdKa",
        "outputId": "ddfbfcde-77a5-4708-c5e2-4ae26a0c029a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zD3mk1b7ckB7",
        "outputId": "5ca1de52-a415-40c1-d282-4c9450c1edca"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8jatwQC7cmLd",
        "outputId": "a40377c3-ed44-4ca0-fe49-20ed52a3bfa6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFiIaa_Jc0l_"
      },
      "source": [
        "#### Adding special context tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YffcQMy9c2oj"
      },
      "source": [
        "We need to modify the tokenizer to handle unknown words. We also need to address the usage and addition of special context tokens that can enhance a model‚Äôs understanding of context or other relevant information in the text. These special tokens can include markers for unknown words and document boundaries, for example. In particular, we will modify the vocabulary and tokenizer, SimpleTokenizerV2, to support two new tokens, <|unk|> and <|endoftext|>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOGqJn9Hc5eC"
      },
      "outputs": [],
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnivByn3c6zt",
        "outputId": "24dc54a6-face-45f9-8337-ac0d80dc266c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('younger', 1127)\n",
            "('your', 1128)\n",
            "('yourself', 1129)\n",
            "('<|endoftext|>', 1130)\n",
            "('<|unk|>', 1131)\n"
          ]
        }
      ],
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vC4r-LUPc88s"
      },
      "outputs": [],
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int\n",
        "            else \"<|unk|>\" for item in preprocessed    # Replaces unknown words by <|unk|> tokens\n",
        "        ]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLu-93_qc-Q0",
        "outputId": "5bf4b554-9e06-462b-9778-b8ba3af7f8f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyR3g8v_c_bO",
        "outputId": "16c585ec-4bea-41c3-f49a-c75cbc6cad2c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.encode(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YpeRJsYudAlM",
        "outputId": "8f0cf18c-df3b-47b6-f6a6-90a6dd3c1650"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7Rd6qv5OkRl"
      },
      "source": [
        "### BPE - Byte-Pair Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypBFCBFHn2cx"
      },
      "source": [
        "Byte per encoding is a common choice. BPE starts by splitting the input by whitespace characters. So a single token will never overlap two word. The vocabulary is initialized with all characters in the training set. New tokens are iteratively created by merging two tokens, which are the most often put next to each other.\n",
        "\n",
        "- GPT-2 used BytePair encoding (BPE) as its tokenizer\n",
        "\n",
        "- it allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words\n",
        "\n",
        "- For instance, if GPT-2's vocabulary doesn't have the word \"unfamiliarword,\" it might tokenize it as [\"unfam\", \"iliar\", \"word\"] or some other subword breakdown, depending on its trained BPE merges\n",
        "\n",
        "- The original BPE tokenizer can be found here: https://github.com/openai/gpt-2/blob/master/src/encoder.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwsNmE8QncMJ"
      },
      "source": [
        "Let‚Äôs explore a more advanced tokenization scheme called Byte Pair Encoding (BPE). The BPE tokenizer was used to train large language models such as GPT-2, GPT-3, and the original model behind ChatGPT.\n",
        "\n",
        "**1. Core Idea of BPE**\n",
        "\n",
        "Instead of memorizing entire words, BPE learns frequently occurring subword units. In other words, it combines commonly seen letters and letter groups to form new words.\n",
        "\n",
        "- For example:\n",
        "\n",
        "If the word \"unhappiness\" is not in the vocabulary, BPE might split it like this:\n",
        "\n",
        "    un + happiness\n",
        "\n",
        "    un + hap + pi + ness\n",
        "\n",
        "This allows the model to break down and understand words it has never seen before.\n",
        "\n",
        "**2. How It Handles Unknown Words (Instead of Using <|unk|>)**\n",
        "\n",
        "If a word is not in the model‚Äôs vocabulary, BPE processes it as follows:\n",
        "\n",
        "- **Checks for Larger Known Chunks:**  \n",
        "  If the word is directly in the vocabulary, it uses that word.  \n",
        "  For example, if \"dog\" is in the vocabulary, it is used as is.\n",
        "\n",
        "- **Breaks the Word into Recognizable Parts:**  \n",
        "  If the whole word is not found, BPE tries to split it into known subword units.  \n",
        "  For example, if \"someunknownPlace\" is unknown:\n",
        "\n",
        "    some ‚úÖ (might be in the vocabulary)  \n",
        "    unknown ‚úÖ (might be in the vocabulary)  \n",
        "    Place ‚úÖ (might be in the vocabulary)\n",
        "\n",
        "- **Uses Even Smaller Subunits:**  \n",
        "  If the parts are still unknown, BPE continues breaking it into smaller chunks:\n",
        "\n",
        "    unk + now + nP + lace\n",
        "\n",
        "If even those are not found, it may encode the word character by character:\n",
        "\n",
        "    s + o + m + e + ...\n",
        "\n",
        "This way, even a completely unknown word can still be processed and understood by splitting it into its smallest components.\n",
        "\n",
        "**3. Example: Processing an Unknown Word**\n",
        "\n",
        "Let‚Äôs say the word \"ChatGPTlikeModel\" is completely unknown. BPE might work as follows:\n",
        "\n",
        "1. It checks the entire word, but it‚Äôs not in the vocabulary.\n",
        "2. It splits the word into the largest known parts:\n",
        "\n",
        "    \"ChatGPT\" ‚úÖ (might be known)  \n",
        "    \"like\" ‚úÖ (might be known)  \n",
        "    \"Model\" ‚úÖ (might be known)\n",
        "\n",
        "If \"ChatGPT\" and \"Model\" are not known, it could break them down into even smaller subunits:\n",
        "\n",
        "    \"Chat\" + \"G\" + \"P\" + \"T\" + \"like\" + \"Model\"\n",
        "\n",
        "So, no word is ever treated as fully \"unknown\". The model finds its internal components and uses them to process the word.\n",
        "\n",
        "**4. Summary**\n",
        "\n",
        "BPE learns frequently used letter groups rather than memorizing full words. When it encounters an unknown word:\n",
        "\n",
        "- It checks if the full word exists in the vocabulary. If not,\n",
        "- It breaks the word into the largest known subword units. If those aren't available,\n",
        "- It further splits the word into smaller components (even down to individual characters).\n",
        "\n",
        "This approach allows the model to handle unknown words without needing a special <|unk|> (unknown word) token.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoeU37tpS-0E"
      },
      "source": [
        "####How Byte-Pair Encoding (BPE) Works?\n",
        "\n",
        "Suppose we have a text corpus with the following four words:  \n",
        "**‚Äúab‚Äù, ‚Äúbc‚Äù, ‚Äúbcd‚Äù and ‚Äúcde‚Äù**.\n",
        "\n",
        "We begin by calculating the frequencies of each byte (character).  \n",
        "Initial vocabulary consists of all the unique characters in the corpus like:  \n",
        "`{‚Äúa‚Äù, ‚Äúb‚Äù, ‚Äúc‚Äù, ‚Äúd‚Äù, ‚Äúe‚Äù}`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyIyt-jFTJPW"
      },
      "source": [
        "#### Step 1: Initialize the Vocabulary\n",
        "\n",
        "`Vocabulary = {\"a\", \"b\", \"c\", \"d\", \"e\"}`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEH86h_3TfBg"
      },
      "source": [
        "#### Step 2: Calculate the frequency of each byte or character in the text corpus:\n",
        "\n",
        "`Frequency = {\"a\": 1, \"b\": 3, \"c\": 3, \"d\": 2, \"e\": 1}`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqt41V8mTe3w"
      },
      "source": [
        "#### Step 3: Find the most frequent pair of two characters\n",
        "\n",
        "`Most frequent pair is \"bc\" with a frequency of 2.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ByYLc85Tpqq"
      },
      "source": [
        "#### Step 4: Merge the pair to create a new subword unit.\n",
        "\n",
        "`Merge \"bc\" to create a new subword unit \"bc\".`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm6CEkDzTpnD"
      },
      "source": [
        "#### Step 5: Update frequency counts of all the bytes or characters that contain the merged pair.\n",
        "\n",
        "Update the frequency counts of all the bytes or characters that contain ‚Äúbc‚Äù:\n",
        "\n",
        "`Frequency = {\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 2, \"e\": 1, \"bc\": 2}`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLLg7_pFTpjn"
      },
      "source": [
        "#### Step 6: Add the new subword unit to the vocabulary\n",
        "\n",
        "`Vocabulary = {\"a\", \"b\", \"c\", \"d\", \"e\", \"bc\"}`\n",
        "\n",
        "Repeat steps 3-6 until the desired vocabulary size is reached."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmQahS7gTpbs"
      },
      "source": [
        "#### Step 7: Represent the text corpus using subword units\n",
        "\n",
        "`Resulting vocabulary consists of the following subword units: {\"a\", \"b\", \"c\", \"d\", \"e\", \"bc\", \"cd\", \"de\",\"ab\",\"bcd\",\"cde\"}.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnVc7oZWUmh2"
      },
      "source": [
        "Original text corpus can be represented using these subword units as follows:\n",
        "\n",
        "`\"ab\" -> \"a\" + \"b\"`\n",
        "\n",
        "`\"bc\" -> \"bc\"`\n",
        "\n",
        "`\"bcd\" -> \"bc\" + \"d\"`\n",
        "\n",
        "`\"cde\" -> \"c\" + \"de\"`\n",
        "\n",
        "This representation helps in reducing the vocabulary size while maintaining the original meaning and structure of the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W2_7XsrStyE"
      },
      "source": [
        "Lets look at anathor exampler\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaH_yXgZXGfc"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1bzxufPYBETaQd_vcF1WNCqRQQHCFC4yv\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vHL_641VokH"
      },
      "source": [
        "- BPE tokenizers break down unknown words into subwords and individual characters:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4e2Y-FPVq-P"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1TH2_iB9vImGUQHYMWrB99qZEl14m5WUt\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbMt0qMRV6e8"
      },
      "source": [
        "Lets look at the code example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrGpSKVhYaiH"
      },
      "source": [
        "A tokenizer is a general component that requires a selected tokenization model to be passed as an argument. It also allows setting some tokenization that will be run on each input text.\n",
        "\n",
        "**Whitespace** pre-tokenization helps to split the text into words.\n",
        "\n",
        "The last thing you will set is the trainer object and the size of a target vocabulary. Practically, you will never go below at least a few thousand, but in this case, 14 should be fine for our experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZiup30nFQs6"
      },
      "outputs": [],
      "source": [
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "bpe_tokenizer = Tokenizer(BPE())\n",
        "bpe_tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "bpe_trainer = BpeTrainer(vocab_size=14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn2NRZDKV9yd"
      },
      "outputs": [],
      "source": [
        "training_data = [\n",
        "    \"\"\"walker walked a long walk\"\"\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpTi13TxWD0M"
      },
      "outputs": [],
      "source": [
        "bpe_tokenizer.train_from_iterator(training_data, bpe_trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vvTb5_WWFfL",
        "outputId": "1d0950f5-3b21-4188-a44b-09eaedf79424"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'d': 1,\n",
              " 'g': 3,\n",
              " 'walke': 13,\n",
              " 'al': 10,\n",
              " 'a': 0,\n",
              " 'r': 8,\n",
              " 'e': 2,\n",
              " 'l': 5,\n",
              " 'k': 4,\n",
              " 'w': 9,\n",
              " 'walk': 12,\n",
              " 'n': 6,\n",
              " 'wal': 11,\n",
              " 'o': 7}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bpe_tokenizer.get_vocab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0svSO7_xZf9U",
        "outputId": "c9b0d7ed-33ce-4866-e0b6-9c07ae741470"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[13, 8, 13, 1, 0, 5, 7, 6, 3, 12]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bpe_tokenizer.encode(\"walker walked a long walk\").ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBhUAPC6WGqw",
        "outputId": "001bc349-db2a-4f0d-ab4e-9f101fc25ddb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['walke', 'r', 'walke', 'd', 'a', 'l', 'o', 'n', 'g', 'walk']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bpe_tokenizer.encode(\"walker walked a long walk\").tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeTOReiOZkBP",
        "outputId": "c305f5be-58a7-4199-c482-4491929b7171"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 4, 9, 8, 9, 2, 8]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bpe_tokenizer.encode(\"akwirw ier\").ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YExIiiNgWJzc",
        "outputId": "9089b1bf-5929-4274-c5d2-7a4980182c07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['a', 'k', 'w', 'r', 'w', 'e', 'r']"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bpe_tokenizer.encode(\"akwirw ier\").tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGOW8xqldzov"
      },
      "source": [
        "### Exercise 1: Prepare your own text dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbgGsgdFgbmN"
      },
      "source": [
        "#### Tokenizer Training & Evaluation Steps (Using BPE)\n",
        "\n",
        "### Step 1: Prepare Training Data\n",
        "- Avoid trivial or very small datasets.\n",
        "- Choose a meaningful, moderately sized corpus (e.g., `verdict.txt` or similar).\n",
        "- Ensure the text is clean and appropriate for language modeling.\n",
        "\n",
        "### Step 2: Initialize and Train the BPE Tokenizer\n",
        "- Use your own `bpe_tokenizer` implementation or a relevant library.\n",
        "- Train it on the selected dataset with a suitable number of merge operations (iterations).\n",
        "- Track and save the generated vocabulary.\n",
        "\n",
        "### Step 3: Test with Logical Sentences\n",
        "- Prepare a few meaningful and grammatically correct sentences.\n",
        "- Tokenize these sentences with your trained tokenizer.\n",
        "- Check:\n",
        "  - Are common words preserved?\n",
        "  - Are subword units logical?\n",
        "\n",
        "### Step 4: Evaluate and Analyze Incorrect Tokenization\n",
        "- If results are odd:\n",
        "  - Was the training data too small?\n",
        "  - Were the number of merge operations insufficient?\n",
        "  - Was there a mismatch between training data and test sentence domain?\n",
        "\n",
        "### Step 5: Test with Random or Artificial Words\n",
        "- Use made-up words such as:\n",
        "  - `kwriodte`\n",
        "  - `blenkratosync`\n",
        "  - `dravolintyxis`\n",
        "- Tokenize these and analyze:\n",
        "  - How does the tokenizer split unknown (OOV) words?\n",
        "  - Does it fall back to character-level tokens?\n",
        "  - Are subword splits meaningful?\n",
        "\n",
        "### Step 6: Compare Different Datasets\n",
        "- Train the tokenizer on multiple datasets (e.g., other `.txt` files or public corpora).\n",
        "\n",
        "You can use this:\n",
        "\n",
        "`!wget https://gist.githubusercontent.com/PrithivirajDamodaran/67af3b055e974a9caec94d96e7591607/raw/0d6babef04eac378e5a5908568ee0ea280809100/big.txt -O big.txt`\n",
        "\n",
        "- Keep training settings constant to ensure fair comparison.\n",
        "- Evaluate using both real and made-up sentences.\n",
        "- Save and compare:\n",
        "  - Vocabulary size\n",
        "  - Tokenization examples\n",
        "- Finally, discuss:\n",
        "  - Which dataset produced the best results?\n",
        "  - Why was it more effective?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u-G1BY3WUaQ"
      },
      "outputs": [],
      "source": [
        "# Libraries\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Your code here\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aptnbPyhUmG"
      },
      "outputs": [],
      "source": [
        "bpe_tokenizer.get_vocab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DeynP2yhZzg",
        "outputId": "6c1a6012-297e-4e68-fa44-3f4bca44f13b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token IDs: [28, 89, 69, 34, 125, 77, 326, 73, 150, 140, 12, 25, 15, 7]\n",
            "Tokens: ['T', 'his', 'is', 'a', 'te', 'st', 'sent', 'en', 'ce', 'for', 'B', 'P', 'E', '.']\n",
            "Decoded Text: T his is a te st sent en ce for B P E .\n"
          ]
        }
      ],
      "source": [
        "text = \"This is a test sentence for BPE.\"\n",
        "\n",
        "\"\"\"\n",
        "Your code here\n",
        "\"\"\"\n",
        "\n",
        "print(\"Decoded Text:\", decoded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmHbK08YhdWJ",
        "outputId": "60ac1b7a-f999-4277-f449-b9684a6368f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token IDs: [35, 45, 73, 44, 105, 72, 52, 58, 47, 36]\n",
            "Tokens: ['b', 'l', 'en', 'k', 'ra', 'to', 's', 'y', 'n', 'c']\n",
            "Decoded Text: b l en k ra to s y n c\n"
          ]
        }
      ],
      "source": [
        "text = \"blenkratosync\"\n",
        "\n",
        "encoded = bpe_tokenizer.encode(text)\n",
        "\n",
        "\"\"\"\n",
        "Your code here\n",
        "\"\"\"\n",
        "\n",
        "print(\"Decoded Text:\", decoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGIRlol00OJC"
      },
      "source": [
        "#### Second Text Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2n34W1w0Rz6",
        "outputId": "96c39be8-0e6f-404f-bdf5-e500d6b328c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-22 14:05:11--  https://gist.githubusercontent.com/PrithivirajDamodaran/67af3b055e974a9caec94d96e7591607/raw/0d6babef04eac378e5a5908568ee0ea280809100/big.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6483133 (6.2M) [text/plain]\n",
            "Saving to: ‚Äòbig.txt‚Äô\n",
            "\n",
            "big.txt             100%[===================>]   6.18M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-04-22 14:05:13 (203 MB/s) - ‚Äòbig.txt‚Äô saved [6483133/6483133]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://gist.githubusercontent.com/PrithivirajDamodaran/67af3b055e974a9caec94d96e7591607/raw/0d6babef04eac378e5a5908568ee0ea280809100/big.txt -O big.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rX9QxHUs0hhI"
      },
      "outputs": [],
      "source": [
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "\"\"\"\n",
        "Your code here\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMW4OLNC0mx2",
        "outputId": "576ad2eb-2408-4c9e-ccbc-0dfbbdf3e397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token IDs: [641, 100, 62, 81, 203, 80, 122, 332, 135, 33, 47, 36, 13]\n",
            "Tokens: ['This', 'is', 'a', 't', 'est', 's', 'ent', 'ence', 'for', 'B', 'P', 'E', '.']\n",
            "Decoded Text: This is a t est s ent ence for B P E .\n"
          ]
        }
      ],
      "source": [
        "text = \"This is a test sentence for BPE.\"\n",
        "\n",
        "\"\"\"\n",
        "Your code here\n",
        "\"\"\"\n",
        "print(\"Decoded Text:\", decoded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOTaTiQA0qV4",
        "outputId": "0a942a6e-3b21-4c57-bcfe-9587a488fcdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token IDs: [306, 98, 72, 79, 97, 196, 86, 75, 64]\n",
            "Tokens: ['bl', 'en', 'k', 'r', 'at', 'os', 'y', 'n', 'c']\n",
            "Decoded Text: bl en k r at os y n c\n"
          ]
        }
      ],
      "source": [
        "text = \"blenkratosync\"\n",
        "\n",
        "\"\"\"\n",
        "Your code here\n",
        "\"\"\"\n",
        "\n",
        "print(\"Decoded Text:\", decoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_khO6rZqrQL"
      },
      "source": [
        "### Tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncD1rLV5hgGE",
        "outputId": "e4c26d48-6f9a-43a0-b549-c6c28f5f3eb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tiktoken==0.7.0\n",
            "  Downloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken==0.7.0) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken==0.7.0) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken==0.7.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken==0.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken==0.7.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken==0.7.0) (2025.1.31)\n",
            "Downloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken==0.7.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xq6_OwNjq0pl",
        "outputId": "e95c1783-1c47-4618-9466-7e610f20815c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tiktoken version: 0.7.0\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "import tiktoken\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASMbjH_kq39r"
      },
      "source": [
        "Once installed, we can instantiate the BPE tokenizer from tiktoken as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o9cPbJBq1ud"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JwXlgiVq6St"
      },
      "source": [
        "The usage of this tokenizer is similar to the SimpleTokenizerV2 we implemented previously via an encode method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9BvDoPlq2-y",
        "outputId": "070b2fa4-184b-40b2-9351-cfeb25a2a5d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ],
      "source": [
        "text = (\n",
        " \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        " \"of someunknownPlace.\"\n",
        ")\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYmqkYRPq7n_",
        "outputId": "1aa892c2-caab-4901-95c0-bdfb81ad55d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
          ]
        }
      ],
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1GJwxiwrHZJ"
      },
      "source": [
        "## Data sampling with a sliding window"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghu-T28_rPu6"
      },
      "source": [
        "The next step in creating the embeddings for the LLM is to generate the input‚Äìtarget pairs required for training an LLM. What do these input‚Äìtarget pairs look like? As we already learned, LLMs are pretrained by predicting the next word in a text, as depicted in figure\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=176WTsvZQtjAKzqHLPnZ1VHy7uOCFUaJA\" width=\"800\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP5yYcxDruU3",
        "outputId": "9d1b2f4d-6a91-4565-b204-2af7c503282a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5145\n"
          ]
        }
      ],
      "source": [
        "with open(\"verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmXECISVr3XM"
      },
      "source": [
        "- For each text chunk, we want the inputs and targets.\n",
        "- Since we want the model to predict the next word, the targets are the inputs shifted by one position to the right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7pUdvfUry5r"
      },
      "outputs": [],
      "source": [
        "enc_sample = enc_text[50:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCgXOJF5r09G",
        "outputId": "f220a879-9916-4ce9-b6d8-45ce5965bcf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x: [290, 4920, 2241, 287]\n",
            "y:      [4920, 2241, 287, 257]\n"
          ]
        }
      ],
      "source": [
        "context_size = 4\n",
        "\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ45ZFb3r7zY"
      },
      "source": [
        "- One by one, the prediction would look like as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIXTRGyir9V5",
        "outputId": "656ec4b4-9132-4c70-ad74-56ab6ffe2f5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[290] ----> 4920\n",
            "[290, 4920] ----> 2241\n",
            "[290, 4920, 2241] ----> 287\n",
            "[290, 4920, 2241, 287] ----> 257\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(context, \"---->\", desired)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBxUzE3PsAMJ",
        "outputId": "fdb602eb-3e15-44f4-efdb-8a04e76e77a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " and ---->  established\n",
            " and established ---->  himself\n",
            " and established himself ---->  in\n",
            " and established himself in ---->  a\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBEoSblasYKR"
      },
      "source": [
        "- We will take care of the next-word prediction in a later chapter after we covered the attention mechanism\n",
        "- For now, we implement a simple data loader that iterates over the input dataset and returns the inputs and targets shifted by one\n",
        "- Install and import PyTorch (see Appendix A for installation tips)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdYYHqlRsX1n",
        "outputId": "a198d0bf-2819-484c-d6be-1ac9fa2e22ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.6.0+cu124\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tays_Aigrhb9"
      },
      "source": [
        "- For this, we use a sliding window approach, changing the position by +1:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igThLSV-rjc2"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1zOKPf9DyxWboounyyhG0rwZO4Mplbijz\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKmctSHisflC"
      },
      "source": [
        "- Create dataset and dataloader that extract chunks from the input text dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfgTzK7wq89R"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last, # drop_last=True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training.\n",
        "        num_workers=num_workers # The number of CPU processesto use for preprocessing\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4OXinVxsjCx"
      },
      "source": [
        "The GPTDatasetV1 class is based on the PyTorch Dataset class and defines how individual rows are fetched from the dataset, where each row consists of a number of token IDs (based on a max_length) assigned to an input_chunk tensor. The target_ chunk tensor contains the corresponding targets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_Wl8Lk3shsZ"
      },
      "outputs": [],
      "source": [
        "with open(\"verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_UgvgjIspt9",
        "outputId": "50fd2130-0ef7-473e-8416-8157606154de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ],
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-GRl6N8sr0C",
        "outputId": "acb11bae-4dfb-4019-b6c4-6ce5ebd1d750"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
          ]
        }
      ],
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faftt235ss-4",
        "outputId": "bd312dd2-9fe7-4728-dd1e-9110910a6bf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n"
          ]
        }
      ],
      "source": [
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED6KvSo8tSd4"
      },
      "source": [
        "# Coding an LLM architecture [[1]](https://www.youtube.com/watch?v=quh7z1q7-uc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIHCN7PZ5KMx"
      },
      "source": [
        "- Models like GPT, Gemma, Phi, Mistral, Llama etc. generate words sequentially and are based on the decoder part of the original transformer architecture\n",
        "- ChatGPT, GPT4, Llama-3, Phi-3, Gemma and Gemini, they all do next word precdiction. So its a sense like the target are shifted by position one.\n",
        "- Therefore, these LLMs are often referred to as \"decoder-like\" LLMs\n",
        "- Compared to conventional deep learning models, LLMs are larger, mainly due to their vast number of parameters, not the amount of code\n",
        "- We'll see that many elements are repeated in an LLM's architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfYun04C6x6n"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1tPvE7PrvNM2CSV5MLAogJITPe_fQpO4A\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzrpEiON68ef"
      },
      "source": [
        "- In the previous, we used small embedding dimensions for token inputs and outputs for ease of illustration, ensuring they neatly fit on the screen\n",
        "- Now we consider embedding and model sizes akin to a small GPT-2 model\n",
        "- Well specifically code the architecture of the smallest GPT-2 model (124 million parameters)\n",
        "- The next section will show how to load pretrained weights into our implementation, which will be compatible with model sizes of 345, 762, and 1542 million parameters\n",
        "-Models like Llama and others are very similar to GPT-2 \"large\" model, since they are all based on the same core concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwwipqCm99dQ"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1UilOjCxm1BTBLOgG17-Xgaa1vwrEJ4aN\" width=\"1200\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghSpls29_Llm"
      },
      "source": [
        "- Embedding layers produce continuous value vectors.\n",
        "\n",
        "- Dropout is no longer popular and is not commonly used in LLMs.\n",
        "\n",
        "- Linear layers are not used in modern architectures. A linear layer is essentially just a simple matrix multiplication."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TTcY-Yn_R6Z"
      },
      "source": [
        "- Configuration details for the 124 million parameter GPT-2 model (GPT-2 \"small\") include:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eb6axhnHAt25",
        "outputId": "b491eda2-5c7d-455a-ebb3-477364b20288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8geVzYxRtWh-"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.0,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIF6JCm1_vla"
      },
      "source": [
        "## Coding the GPT model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD39OX3d_4Xj"
      },
      "source": [
        "- We are almost there: now let's plug in the transformer block into the architecture we coded at the very beginning of this notebook so that we obtain a useable GPT architecture\n",
        "- Note that the transformer block is repeated multiple times; in the case of the smallest 124M GPT-2 model, we repeat it 12 times:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXLdLvJM_73-"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1GbDioSyY_YJAOWeJxGzYI8lV-hMaeFtP\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z2tXg81ADzk"
      },
      "source": [
        "- The corresponding code implementation, where `cfg[\"n_layers\"] = 12`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WsCNLSDnAarF"
      },
      "outputs": [],
      "source": [
        "#@title Required codes\n",
        "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
        "# Source for \"Build a Large Language Model From Scratch\"\n",
        "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
        "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
        "\n",
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)  # optional projection\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZBYsh_p_U75"
      },
      "outputs": [],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTa8LeJTCw7l"
      },
      "source": [
        "- Using the configuration of the 124M parameter model, we can now instantiate this GPT model with random initial weights as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6scdndSAReb",
        "outputId": "51322f34-3b55-4944-fddc-21e2e1724270"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "batch = []\n",
        "\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "print(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z91JB6XCCzB4",
        "outputId": "69887cc6-d8ee-477d-c022-e8dad2687241"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input batch:\n",
            " tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n",
            "\n",
            "Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[ 6.4165e-02,  2.0443e-01, -1.6945e-01,  ...,  1.7887e-01,\n",
            "           2.1921e-01, -5.8153e-01],\n",
            "         [ 3.7736e-01, -4.2545e-01, -6.5874e-01,  ..., -2.5050e-01,\n",
            "           4.6553e-01, -2.5760e-01],\n",
            "         [ 8.8996e-01, -1.3770e-01,  1.4748e-01,  ...,  1.7770e-01,\n",
            "          -1.2015e-01, -1.8902e-01],\n",
            "         [-9.7276e-01,  9.7338e-02, -2.5419e-01,  ...,  1.1035e+00,\n",
            "           3.7639e-01, -5.9006e-01]],\n",
            "\n",
            "        [[ 6.4165e-02,  2.0443e-01, -1.6945e-01,  ...,  1.7887e-01,\n",
            "           2.1921e-01, -5.8153e-01],\n",
            "         [ 1.3433e-01, -2.1289e-01, -2.7020e-02,  ...,  8.1153e-01,\n",
            "          -4.7410e-02,  3.1186e-01],\n",
            "         [ 8.9996e-01,  9.5396e-01, -1.7896e-01,  ...,  8.3053e-01,\n",
            "           2.7657e-01, -2.4577e-02],\n",
            "         [-9.3073e-05,  1.9390e-01,  5.1217e-01,  ...,  1.1915e+00,\n",
            "          -1.6431e-01,  3.7046e-02]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "out = model(batch)\n",
        "print(\"Input batch:\\n\", batch)\n",
        "print(\"\\nOutput shape:\", out.shape)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3_ZvNM2C3HR"
      },
      "source": [
        "- We will train this model in the next part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3rbVNypC6ic"
      },
      "source": [
        "## Generating text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV9BpfnuC9KW"
      },
      "source": [
        "- LLMs like the GPT model we implemented above are used to generate one word at a time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGxR9ii7C_M4"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1N-lvGsKV62IfbN3QoE4ZLzniGni8Ncpy\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzeTlTTODF31"
      },
      "source": [
        "- The following `generate_text_simple` function implements greedy decoding, which is a simple and fast method to generate text\n",
        "- In greedy decoding, at each step, the model chooses the word (or token) with the highest probability as its next output (the highest logit corresponds to the highest probability, so we technically wouldn't even have to compute the softmax function explicitly)\n",
        "- The figure below depicts how the GPT model, given an input context, generates the next word token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzbNN3-KDDiq"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1Hqs9c1NEQf28wxcV3BUKOcFHXOKNkKOE\" width=\"800\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJppjwGrC1lt"
      },
      "outputs": [],
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB-9aiI4DQT5",
        "outputId": "ca4dba5d-4608-47ee-9e1e-2d35b220cfff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "encoded: [15496, 11, 314, 716]\n",
            "encoded_tensor.shape: torch.Size([1, 4])\n"
          ]
        }
      ],
      "source": [
        "start_context = \"Hello, I am\"\n",
        "\n",
        "encoded = tokenizer.encode(start_context)\n",
        "print(\"encoded:\", encoded)\n",
        "\n",
        "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xbt_DaEADoaL",
        "outputId": "67740391-59c3-4e62-d859-d35ca251d626"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
            "Output length: 10\n"
          ]
        }
      ],
      "source": [
        "out = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=encoded_tensor,\n",
        "    max_new_tokens=6,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output:\", out)\n",
        "print(\"Output length:\", len(out[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CKr5YkfDplf",
        "outputId": "7d3172d2-8c9a-49ce-a010-5777f2d5c937"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, I am Featureiman Byeswickattribute argue\n"
          ]
        }
      ],
      "source": [
        "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
        "print(decoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgxY1eiRDtNv"
      },
      "source": [
        "### Exercise 2: Examine the above result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF2g0a9TD9xx"
      },
      "source": [
        "- **As you can see above, after the \"Hello, I am\" prompt, there are some meaningless results. Test with other examples to see if this is always the case. If it is also nonsensical, what could be the reason for this?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sstsUJgpEsxw"
      },
      "source": [
        "# Pretraining [[1]](https://www.youtube.com/watch?v=quh7z1q7-uc), [[3]](https://www.deeplearning.ai/short-courses/pretraining-llms/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGPIGmIiqZ1i"
      },
      "source": [
        "Pre-training is the first phase of training on LLM, where the model learns to generate a text by repeatedly predicting the next word. It learns how to do this by using very large amounts of unstructed text data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj_KCeqdtglJ"
      },
      "source": [
        "Let's experience a different among pre-trained models and fine tuned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vvIazlJuHLS"
      },
      "source": [
        "### **Generate Python samples with pretrained general model**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWTogiMK0YWU"
      },
      "source": [
        "- Here we'll be using a model collad TinySolar for our experience. To load this model, we will be using auto model for causal LLM from transformers.\n",
        "\n",
        "- TinySolar-248m-4k is a **small decoder-only model** with 248M parameters (similar in scale to GPT2) and a 4096 token context window.\n",
        "\n",
        "- You'll load the model in three steps:\n",
        "\n",
        "    - Specify the path to the model in the Hugging Face model library\n",
        "    - Load the model using AutoModelforCausalLM in the transformers library\n",
        "    - Load the tokenizer for the model from the same model path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcuv1BXIExAn"
      },
      "outputs": [],
      "source": [
        "# Set a seed for reproducibility\n",
        "import torch\n",
        "\n",
        "def fix_torch_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "fix_torch_seed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "ea589fe35ac440f8bbbcaa12886ff96a",
            "5237098fdf514c968050f230330bee7b",
            "6f2dd6c63066428581458dd277a89a79",
            "feae6fa0a4af4976bdf82fe0497d52f9",
            "419af8dc01194e4d97aa1db204d99206",
            "4ce740f47cea46459778af40183b5b39",
            "0a0e0928bdf24053b39306ce21920e91",
            "997570e2b12c46cbb810b3d1bf5f92e8",
            "3d37b37fbc5d43fdaf2a069d092aa8d8",
            "b4c4c7fe239c49929cc2e565f3de6c00",
            "500687cb5c4f40049a1354b3d8347212",
            "93e4a13b6da94d738ae260f25609b03a",
            "7b631c995e244eb8b02c315a2cc2d5e7",
            "a0991a30480149de87f7446363ecc256",
            "c2ec3490fcfa4607954285994fbcfbd8",
            "8ecbab175e6c48fdacc3a715b30d1b2d",
            "ddad8eb7e7eb46ae9258a16a84cd8300",
            "3eacc69a26be4e3f8678de7277f7b1cc",
            "449e3e7d939b4144bfd999b2697a922d",
            "8f9a891ea6954a778c6c3c85ae9400ea",
            "d89a5b617c204dd1ae669308fda01a69",
            "af4963d6b9d14ed692e5161a4b0d2695",
            "52e606035eab40b782f167d08fec039d",
            "ce158e51dc6743868b785c629e1d6371",
            "acbf27cc879f48adb4082265e360acde",
            "bd9d7fdfefd14ffdb1fbccb9d6f9dcd8",
            "437ad27416dc42688c721ccbb0920347",
            "9f5f39f64cca4c77a0fa22434fb65e47",
            "cc81faed6cd44f8596c56e23bbbf37a3",
            "62b6e11a20e9485abf3a27c45f518224",
            "94c1886f40a4405e9ec29eb3374640b5",
            "32c4fd5308cf468d9436ae2749c658cf",
            "399063fd819b402481e3162aaa7529a0"
          ]
        },
        "id": "Gj_AzzhutwB2",
        "outputId": "7449cc70-26e8-4f94-a1f7-9ad4560368dd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea589fe35ac440f8bbbcaa12886ff96a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af4963d6b9d14ed692e5161a4b0d2695",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "399063fd819b402481e3162aaa7529a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model_path_or_name = 'upstage/TinySolar-248m-4k'  # Adjust this path as per your local setup\n",
        "\n",
        "tiny_general_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path_or_name,\n",
        "    device_map=\"cpu\",  # Change to \"auto\" for GPU if available\n",
        "    torch_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "916345a464c34756ba7ae815f8540bc8",
            "4b10352386d2455ea74b2163a6413eaf",
            "45d8e1575e0845cfade879aca2be9a26",
            "ecad2bb4f01c41f4b5b7f58b473a2e34",
            "8a1d0acb8d634d5aa2211b5b3870cfa7",
            "1943d4c9f95c4aada9643e23091627e6",
            "157cf0543aca4e528c24de5f9e46150d",
            "6fd5e5c7b7df470a963cb06ff50ea221",
            "95f17f734cb344e886dd0ed69ce19c65",
            "c371139aae3644eab9e7fc47e0610521",
            "74a9eb684b094a2c8473787ebbfbc656",
            "54982052e1e84ee5bc8f74b5cdda62f5",
            "78de3a26cd3b4e8a83ac2fd6fad670a6",
            "6ca828e9a66445c4b00ed58bbc2f4a72",
            "98c5dc61dd2d49d18dc004a12b8ad404",
            "ce9e36a6c8f34b59a8eb0e9a5d8b2ca4",
            "014e3e7ec8c547b88cdd7c070aa3863d",
            "5b039e4d5d534d17b58d53246743eb4a",
            "37242a7e68ef4f6e90ca2a022c0ade6a",
            "fe963150fcd84258966e7193f17b4249",
            "4c9713b2452c49c4b5fc6979b179fa0d",
            "0d930f4785ad4a588210921f0198f5be",
            "74b13f99530742168d2436079d0a3f81",
            "611506548d154d819ab6c7f9c0e0dea4",
            "0d3c7a0a351f4250b9bd422b1943eb6b",
            "2f94d65d8f8a49599d0c23f9caa342d3",
            "31431ba1f7d74b5c80a9ca5b137f5d59",
            "2857caf7c6ce4bf58a634050741107d5",
            "ff160a9bf6c14b859ff0c5d01c8adc1d",
            "f3be3ac885a74ec59f8c53a76a052d89",
            "48fa62f78e034f6095e3167af51150e0",
            "e6b5b8439dd24aa5b6d58f5331ecfa5a",
            "323dffde90834e5ea2e1fab628b5b4ab",
            "42da3061bd094cc1893e188a643d9da2",
            "d8e016547e924a1db5934db3b07a7657",
            "90cbaf4f0b464a58996c8f22f9b60d23",
            "a97676b87bc04bf4be236843b5a8298e",
            "977ab5e11fa548c98e3a36b555a7071e",
            "35c30653ccb446cb81d5463b759cc075",
            "07354cd832c84e79b6c3fe64b0b95ab9",
            "44baf6f301bd4a5eb1032d7fb943883a",
            "d568756213da4359aa72cc06176d3c38",
            "7cab87de21c34cec96b9dae131671761",
            "4db024ca06584b72b35db5dc64a2c453"
          ]
        },
        "id": "N2mHL5n0tv7_",
        "outputId": "64382df5-c7df-4086-c0a0-dafba4237cb7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "916345a464c34756ba7ae815f8540bc8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/966 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d930f4785ad4a588210921f0198f5be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "323dffde90834e5ea2e1fab628b5b4ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4db024ca06584b72b35db5dc64a2c453",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tiny_general_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_path_or_name\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txWHvzbJwApc"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "streamer = TextStreamer(\n",
        "    tiny_general_tokenizer,\n",
        "    skip_prompt=True, # If you set to false, the model will first return the prompt and then the generated text\n",
        "    skip_special_tokens=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks96K5kQw33r"
      },
      "source": [
        "Use the model to write a python function called `find_max()` that finds the maximum value in a list of numbers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySWUPIjWtvq1",
        "outputId": "84869d9e-b999-40b6-e09b-0bbd9161651e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   \"\"\"\n",
            "   Returns the number of times a user has been added to the list.\n",
            "   \"\"\"\n",
            "   num = len(list)\n",
            "   if len(list[0]) > 1:\n",
            "       return int(len(list))\n",
            "   else:\n",
            "       return int(len(list))\n",
            "\n",
            "\n",
            "def get_user_id(user_id, user_name):\n",
            "   \"\"\"\n",
            "   Returns the user id for this user.\n",
            "   \"\"\"\n",
            "   return user_id\n",
            "\n",
            "\n",
            "def get_user_id_from_user(user_id, user_name):\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt =  \"def find_max(numbers):\"\n",
        "\n",
        "inputs = tiny_general_tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "inputs = tiny_general_tokenizer(\n",
        "    prompt, return_tensors=\"pt\"\n",
        ").to(tiny_general_model.device)\n",
        "\n",
        "streamer = TextStreamer(\n",
        "    tiny_general_tokenizer,\n",
        "    skip_prompt=True, # Set to false to include the prompt in the output\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "outputs = tiny_general_model.generate(\n",
        "    **inputs,\n",
        "    streamer=streamer,\n",
        "    use_cache=True,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=False,\n",
        "    temperature=0.0,\n",
        "    repetition_penalty=1.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJQJhhHcxyPU"
      },
      "source": [
        "Seems like Python code at a glimpse, but if you look closely, the result is just random. So there are commoents here and a return format, but there are no calculations inside.  \n",
        "\n",
        "- Let's run the code given by this model and see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "AnILNMUZyVJM",
        "outputId": "f549aad4-79d2-456f-b648-0518d7e16b6e"
      },
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-1-97b94f0ce8ba>, line 5)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-97b94f0ce8ba>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    if len(list[0]) > 1:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "numbers = [0, 100, -50, 46, 124, 4, 53]\n",
        "\n",
        "def find_max(numbers):\n",
        "  num = len(list)\n",
        "   if len(list[0]) > 1:\n",
        "       return int(len(list))\n",
        "   else:\n",
        "       return int(len(list))\n",
        "\n",
        "find_max(numbers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rtq_52DZy77O"
      },
      "source": [
        "- Let's try a new model. Here you'll use a version of TinySolar-248m-4k that has been further pretrained **(a process called continued pretraining)** on a large selection of python code samples.\n",
        "- The main difference with the previous model is that the data set is at least 100 times bigger\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "0f8785bcd326446abd2fcb325bc27665",
            "89d106f463a3494dbb87e29ed21113c7",
            "45763aa6594849cd84238643b6e4ccd6",
            "f44b0adf10da431cbadcb24b8f4ed9ef",
            "eba539fb01f04669b2804b6700dbdc1e",
            "1396da3d20584f98969d871dc1e106d9",
            "f102897934f54c5ebaa4a3577d524f88",
            "5d3bb1f5b5224e90a69c5c33607ccd43",
            "3241ec0dfeef4574a19590bb0742a4fc",
            "4d76079e7f9842efaa53779315138fcf",
            "9e871c1747034d5dab0d11907df7364d",
            "707cda2254f847dab875597a0f1cc729",
            "89d9561235f34c8d9bc5d27d562d31c7",
            "dea1d2192ab84e4b87360c9c83324d16",
            "799fa6a283b34de488963af35a0f7706",
            "7f89fae5ab7c4b84802a42f9f8ebfa24",
            "e054fc22f3b64330a863d0470e4a6085",
            "9deebfefd0af47ff89738f6f0cf07a8c",
            "4860f2a6ccba40db865000ca3857702a",
            "e83206ac6a6842cdb43c1ceeb4abc976",
            "ed212b3b70ad4aa4a432628406b231be",
            "5d9512a775484ff4a8042cb65fc88a83",
            "dcfef85326cc4c869c420c8aca331d51",
            "eebaefe55eaa42d99e11ce8db624aebb",
            "5aa77b75eba54b919299f33e44aecfeb",
            "1b750616cffc4b1a97417c239e036b61",
            "dbce45180a1f45c1b110349787f5adc1",
            "9910e99763ec420d91de1ca07cadaf32",
            "5d2dcff9990240f89da2d24cdcd86bed",
            "b02d1c2cea3f44f49e7e0ef5eb7c25ee",
            "bdd87899063a42b9b20b817e36d29066",
            "198e544f642540d898637c1ffec9d92b",
            "32060ab50e2047dc8ac7b3cf58acdf58"
          ]
        },
        "id": "bKsgXRBlvxXY",
        "outputId": "c5fe7f65-98d8-4faf-fc3f-94cf89b4dd16"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f8785bcd326446abd2fcb325bc27665",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/639 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "707cda2254f847dab875597a0f1cc729",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dcfef85326cc4c869c420c8aca331d51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_path_or_name = 'upstage/TinySolar-248m-4k-py'  # Adjust this path as per your local setup\n",
        "\n",
        "tiny_general_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path_or_name,\n",
        "    device_map=\"cpu\",  # Change to \"auto\" for GPU if available\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "streamer = TextStreamer(\n",
        "    tiny_general_tokenizer,\n",
        "    skip_prompt=True, # If you set to false, the model will first return the prompt and then the generated text\n",
        "    skip_special_tokens=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3Q016FWwTFG",
        "outputId": "ac3771dc-c060-4720-a563-af230dd0aacd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   \"\"\"Find the maximum number of numbers in a list.\"\"\"\n",
            "   max = 0\n",
            "   for num in numbers:\n",
            "       if num > max:\n",
            "           max = num\n",
            "   return max\n",
            "\n",
            "\n",
            "def get_min_max(numbers, min_value=1):\n",
            "   \"\"\"Get the minimum value of a list.\"\"\"\n",
            "   min_value = min_value or 1\n",
            "   for num in numbers:\n",
            "       if num < min_value:\n",
            "           min_value = num\n",
            "   return min_value\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt =  \"def find_max(numbers):\"\n",
        "\n",
        "inputs = tiny_general_tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "inputs = tiny_general_tokenizer(\n",
        "    prompt, return_tensors=\"pt\"\n",
        ").to(tiny_general_model.device)\n",
        "\n",
        "streamer = TextStreamer(\n",
        "    tiny_general_tokenizer,\n",
        "    skip_prompt=True, # Set to false to include the prompt in the output\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "outputs = tiny_general_model.generate(\n",
        "    **inputs,\n",
        "    streamer=streamer,\n",
        "    use_cache=True,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=False,\n",
        "    temperature=0.0,\n",
        "    repetition_penalty=1.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mltn_KRLxqvk",
        "outputId": "e8666535-efde-4020-b0f4-3789aa6f0aaa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "124"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "numbers = [0, 100, -50, 46, 124, 4, 53]\n",
        "\n",
        "def find_max(numbers):\n",
        "   max = 0\n",
        "   for num in numbers:\n",
        "       if num > max:\n",
        "           max = num\n",
        "   return max\n",
        "\n",
        "find_max(numbers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmFI2sRe0D4U"
      },
      "source": [
        "- The function finally makes sense"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R7Metd20vYQ"
      },
      "source": [
        "### Let's focus on our GPT-2 Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIsbxlibnJz7"
      },
      "source": [
        "- In the previous section, we saw the importance of pretraining. Now, let's train the GPT-2 model we worked on earlier using our own dataset and observe the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkNRHsgX053L"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1uRPjMiN1N-VyX-FHngnSZADI7wRURvgx\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ossNtrVCkFnQ"
      },
      "source": [
        "**Training GPT-2 small model from scratch in Hugging Face (with Pytorch backend)**\n",
        "\n",
        "- Let us train a GPT-2 (small,124 million parameters) model from scratch using the Hugging Face library.\n",
        "- Instead of using WebText dataset (due to limited computing resources). Here we will use the verdict.txt dataset that we used before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylx9voL9Vgim",
        "outputId": "8eba3567-4ed6-4fc5-975d-c3ae9174ab48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-23 07:13:05--  https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20479 (20K) [text/plain]\n",
            "Saving to: ‚Äòverdict.txt‚Äô\n",
            "\n",
            "\r",
            "verdict.txt           0%[                    ]       0  --.-KB/s               \r",
            "verdict.txt         100%[===================>]  20.00K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-04-23 07:13:06 (28.9 MB/s) - ‚Äòverdict.txt‚Äô saved [20479/20479]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt \\\n",
        "     -O verdict.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fo5U_JrmVAaH"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "R0zbuUQUbG2u"
      },
      "outputs": [],
      "source": [
        "#@title Code to avoid unnecessary output\n",
        "import os\n",
        "import warnings\n",
        "import logging\n",
        "from transformers import logging as hf_logging\n",
        "\n",
        "# 1) Set environment variables to silence various logs\n",
        "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"           # show only error messages from Transformers\n",
        "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"    # disable advisory warnings\n",
        "os.environ[\"TQDM_DISABLE\"] = \"1\"                          # disable tqdm progress bars globally\n",
        "\n",
        "# 2) Suppress all warnings from Python\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# 3) Set Hugging Face Transformers logging level to ERROR\n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "hf_logging.set_verbosity_error()                          # suppress internal logging from Transformers\n",
        "hf_logging.disable_progress_bar()                         # disable download/progress bars\n",
        "\n",
        "# 4) Set Hugging Face Hub logs to ERROR level as well\n",
        "logging.getLogger(\"huggingface_hub\").setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_K5hgnEolUi"
      },
      "source": [
        "### Using GPT to generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEI0ICMDon6p"
      },
      "source": [
        "- We initialize a GPT model using HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwRG9WHUkcN7"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,        # Vocabulary size\n",
        "    \"n_positions\": 256,         # context_length\n",
        "    \"n_embd\": 768,              # Embedding dimension\n",
        "    \"n_head\": 12,               # Number of attention heads\n",
        "    \"n_layer\": 12,              # Number of layers\n",
        "    \"attn_pdrop\": 0.1,          # Dropout rate\n",
        "    \"resid_pdrop\": 0.1,\n",
        "    \"embd_pdrop\": 0.1,\n",
        "    \"add_cross_attention\": False,\n",
        "    \"use_cache\": True\n",
        "}\n",
        "\n",
        "config = GPT2Config(**GPT_CONFIG_124M)\n",
        "model = GPT2LMHeadModel(config) # define the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t55qglGBloB9"
      },
      "source": [
        "- We use dropout of 0.1 above, but it's relatively common to train LLMs without dropout nowadays\n",
        "- We reduce the context length (`context_length`) of only 256 tokens to reduce the computational resource requirements for training the model, whereas the original 124 million parameter GPT-2 model used 1024 tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EFFSjiGoxM1"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1gc7zErdEdT1GYNdiUlag9od_u4WmrQlk\" width=\"800\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_SEUzZrnvpZ",
        "outputId": "df56da0d-fac8-4a73-e881-f5641197771b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Every effort moves you nutrit nutrit nutritOutOutOutOutOut z z\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "# .Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.model_max_length = GPT_CONFIG_124M[\"n_positions\"]\n",
        "\n",
        "# Input-output conversion functions\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "    return encoded\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    return tokenizer.decode(token_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Simple text generation function\n",
        "def generate_text_simple(model, idx, max_new_tokens=10, context_size=256):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -context_size:]\n",
        "            logits = model(idx_cond).logits\n",
        "            next_token_logits = logits[:, -1, :]\n",
        "            probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
        "            next_token = torch.argmax(probs, dim=-1).unsqueeze(0)\n",
        "            idx = torch.cat([idx, next_token], dim=1)\n",
        "    return idx\n",
        "\n",
        "# Usage\n",
        "start_context = \"Every effort moves you\"\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"n_positions\"]\n",
        ")\n",
        "\n",
        "# Result\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H56RLA89oOev"
      },
      "source": [
        "- As we can see above, the model does not produce good text because it has not been trained yet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ihTOj5Qo1u4"
      },
      "source": [
        "### Training an LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SP7WcH3o7vq"
      },
      "source": [
        "- **Preparing the dataset loaders**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "94c3263e9f004b498ebd3281d8e920fa",
            "5fb5bba591324b1d901f2fa6ba433537",
            "b99508cdc24546f2af5d14a1efef7b3d",
            "7d5482d6ae604719a8878b911eb25aa1",
            "c20ae1b419df4436ab4dc5cc8ef31034",
            "ec5ec96509714659ac33aab2230b6e43",
            "8fd14044b8fe49138aa3b48479b4d29f",
            "ba7b86b365b9489e8e07345c79cdc620",
            "dfd3fbff6f86417fb291610d0eba55cd",
            "f35dd06a6abf4e52a5424ead1de31d1a",
            "d5e6f4a9fa0a472b810482bb6f253b22"
          ]
        },
        "id": "Nlv0rLFoSBq5",
        "outputId": "c5244b0a-cfc7-4452-da33-1128bcfef866"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94c3263e9f004b498ebd3281d8e920fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# 'data.txt' is loaded such that each line is treated as a separate training example\n",
        "raw_ds = load_dataset(\"text\", data_files={\"train\": \"verdict.txt\"})[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60y5kkcZTK3t"
      },
      "outputs": [],
      "source": [
        "# %10 for validation\n",
        "splits = raw_ds.train_test_split(test_size=0.1, seed=42)\n",
        "train_ds = splits[\"train\"]\n",
        "val_ds   = splits[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opiJNOYgTkE2"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# First, split the dataset into 80% training and 20% (dev + test)\n",
        "tmp = raw_ds.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "# Then, split the remaining 20% into 10% validation and 10% test\n",
        "dev_test = tmp[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
        "\n",
        "# Construct the final dataset dictionary\n",
        "ds = DatasetDict({\n",
        "    \"train\": tmp[\"train\"],\n",
        "    \"validation\": dev_test[\"train\"],\n",
        "    \"test\": dev_test[\"test\"]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_Yz_HquUPUA"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize(examples):\n",
        "    tok = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "    tok[\"labels\"] = tok[\"input_ids\"].copy()\n",
        "    return tok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "6d2037f873e24ad18797212e4beedb3c",
            "36703958a0664345be2b06b9590086d4",
            "2cca84e5a2244deda0a2e848c92695f8",
            "e8abdb8b6e19407d98326133d9efc16e",
            "1b619cf6bfbb4821917c9c7c5ed43dc7",
            "1f42b33a3eae4d76afedeec3685497af",
            "eb29c66c14604a2fbc5119dcc1e5ca25",
            "262e906b99f3486e9a7228e7df9109a9",
            "d4ecd07c1ece4828a1c428d4c2427248",
            "df849c78a2e148078f5e751a870ba589",
            "772d492526434bc39ebfed41d959df22",
            "a640092d5ed746e8957b2dd60b76f937",
            "c3b6d71a73ec40c4a0cfb5a1dd543607",
            "fe78c24264cf48f1b3befdbf0fd9547d",
            "c7dce2c65a494424a85321a9052e5c17",
            "915290cd2a5e46e8aba3600754bfa8d5",
            "2415df8ef65e474ebd5b25f96b6b1565",
            "4bcf52bb89154535b310e0e7f9b63102",
            "052af3e6e3194c37af49660a763ce735",
            "3208a43cf9f24f1e9b3ac4d88c8d1626",
            "24fd047eeb934528ba4c70ff141646eb",
            "6bd86b8c65af4b21acba4e9dc866ac51"
          ]
        },
        "id": "R5zHlELDURBv",
        "outputId": "7e931f7b-4101-489e-f032-d22ea8499844"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d2037f873e24ad18797212e4beedb3c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/148 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6bd86b8c65af4b21acba4e9dc866ac51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/17 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Map tokenization\n",
        "train_tok = train_ds.map(tokenize, batched=True)\n",
        "val_tok   = val_ds.map(tokenize, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R6av92P1TNHI",
        "outputId": "30f99787-a659-4395-d227-6fb49b6bf90a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 148\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 370\n",
            "  Number of trainable parameters = 123,849,984\n",
            "The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='370' max='370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [370/370 01:05, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.633500</td>\n",
              "      <td>2.046473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.421900</td>\n",
              "      <td>1.794901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.386100</td>\n",
              "      <td>1.680541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.324700</td>\n",
              "      <td>1.617880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.416500</td>\n",
              "      <td>1.596488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.321900</td>\n",
              "      <td>1.601348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.357000</td>\n",
              "      <td>1.598784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.115300</td>\n",
              "      <td>1.619542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.715800</td>\n",
              "      <td>1.623800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.869800</td>\n",
              "      <td>1.631291</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 17\n",
            "  Batch size = 4\n",
            "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 17\n",
            "  Batch size = 4\n",
            "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 17\n",
            "  Batch size = 4\n",
            "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 17\n",
            "  Batch size = 4\n",
            "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 17\n",
            "  Batch size = 4\n",
            "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 17\n",
            "  Batch size = 4\n",
            "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 17\n",
            "  Batch size = 4\n",
            "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 17\n",
            "  Batch size = 4\n",
            "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 17\n",
            "  Batch size = 4\n",
            "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 17\n",
            "  Batch size = 4\n",
            "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Saving model checkpoint to ./final_result\n",
            "Configuration saved in ./final_result/config.json\n",
            "Configuration saved in ./final_result/generation_config.json\n",
            "Model weights saved in ./final_result/model.safetensors\n",
            "tokenizer config file saved in ./final_result/tokenizer_config.json\n",
            "Special tokens file saved in ./final_result/special_tokens_map.json\n",
            "tokenizer config file saved in ./final_result/tokenizer_config.json\n",
            "Special tokens file saved in ./final_result/special_tokens_map.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('./final_result/tokenizer_config.json',\n",
              " './final_result/special_tokens_map.json',\n",
              " './final_result/vocab.json',\n",
              " './final_result/merges.txt',\n",
              " './final_result/added_tokens.json',\n",
              " './final_result/tokenizer.json')"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set training arguments (configuration)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",                    # directory to save model checkpoints and results\n",
        "    report_to=\"tensorboard\",                   # report logs to TensorBoard\n",
        "    logging_dir=\"./tb_logs\",                   # directory for saving TensorBoard logs\n",
        "    logging_strategy=\"steps\",                  # log every few steps\n",
        "    logging_steps=10,                          # log every 10 steps\n",
        "    per_device_train_batch_size=4,             # batch size for training per device\n",
        "    per_device_eval_batch_size=4,              # batch size for evaluation per device\n",
        "    num_train_epochs=10,                       # number of training epochs\n",
        "    eval_strategy=\"epoch\",                     # evaluate the model at the end of each epoch\n",
        "    save_strategy=\"no\",                     # save the model at the end of each epoch\n",
        "    disable_tqdm=False,                        # enable progress bars\n",
        "    log_level=\"info\"                           # log level set to 'info' (to log train loss and other details)\n",
        ")\n",
        "\n",
        "# Initialize Trainer with the specified model and training arguments\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tok,                   # training dataset (tokenized)\n",
        "    eval_dataset=val_tok,                      # evaluation dataset (tokenized)\n",
        "    tokenizer=tokenizer                        # tokenizer for preprocessing text\n",
        ")\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(\"./final_result\")      # model weights + config\n",
        "tokenizer.save_pretrained(\"./final_result\")  # tokenizer files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "A45C6GbvTPHH",
        "outputId": "483a4b10-29f3-4dd4-8389-9be305bd51c3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history = trainer.state.log_history\n",
        "df = pd.DataFrame(history)\n",
        "\n",
        "train_df = df[df[\"loss\"].notna()]\n",
        "eval_df  = df[df[\"eval_loss\"].notna()]\n",
        "\n",
        "plt.plot(train_df[\"epoch\"], train_df[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(eval_df[\"epoch\"],  eval_df[\"eval_loss\"], label=\"eval_loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Train vs Eval Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm5W0ti_pWvi"
      },
      "source": [
        "- Looking at the results above, we can see that the model starts out generating incomprehensible strings of words, whereas towards the end, it's able to produce grammatically more or less correct sentences\n",
        "\n",
        "- However, based on the training and validation set losses, we can see that the model starts overfitting\n",
        "\n",
        "- If we were to check a few passages it writes towards the end, we would find that they are contained in the training set verbatim -- it simply memorizes the training data\n",
        "\n",
        "- Also note that the overfitting here occurs because we have a very, very small training set, and we iterate over it so many times\n",
        "\n",
        "    - The LLM training here primarily serves educational purposes; we mainly want to see that the model can learn to produce coherent text\n",
        "    - Instead of spending weeks or months on training this model on vast amounts of expensive hardware, we load pretrained weights later"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWXwrpzJpp7x"
      },
      "source": [
        "## Exercise 3: Load the pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNPK3hIbtYve"
      },
      "source": [
        "### Step 1:\n",
        "Define the tokenizer and model using `AutoModelForCausalLM` and `AutoTokenizer`.\n",
        "\n",
        "### Step 2:\n",
        "Use Hugging Face's `pipeline` function to create a pipeline for text generation.\n",
        "\n",
        "### Step 3:\n",
        "Define the generator for output. The generator will take the following parameters:\n",
        "- `prompt`: The starting text for the model to generate from.\n",
        "- `max_length`: The maximum token length for the generated text.\n",
        "- `num_return_sequences`: The number of text sequences to return.\n",
        "\n",
        "Afterwards, analyze the results and check if the words generated by the model are present in the `verdict.txt` file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VO9imRIiWcmf",
        "outputId": "540bae68-431b-4cf8-eadd-7c3664c9feb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Option 1 ===\n",
            "Every effort moves you of the to me. I was that I had the room, and in the picture.\n",
            "\n",
            "=== Option 2 ===\n",
            "Every effort moves you with a he had given with a little of the room, in the course arm.\n",
            "\n",
            "=== Option 3 ===\n",
            "Every effort moves you at the Mrs. Rickham the was his eyes of his to the. He was him. But, it to me--she's an it's in the fact, he had me. Gisburn's not to have the was no one in a little: \" not till any of the I was not such. He of the\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "import logging\n",
        "from transformers import logging as hf_logging\n",
        "\n",
        "# Step 1:\n",
        "model_dir = \"...\"\n",
        "tokenizer = \"...\"\n",
        "model     = \"...\"\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "# Step 2:\n",
        "generator = pipeline(\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        ")\n",
        "\n",
        "prompt = \"Every effort moves you\"\n",
        "# Step 3\n",
        "outputs = generator(\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        ")\n",
        "\n",
        "for i, out in enumerate(outputs):\n",
        "    print(f\"=== Option {i+1} ===\\n{out['generated_text']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxJmyrertgzq"
      },
      "source": [
        "## Exersice 4 (Optional): Train the LLM with larger texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fyG4z_-uIi_"
      },
      "source": [
        "`!wget https://gist.githubusercontent.com/PrithivirajDamodaran/67af3b055e974a9caec94d96e7591607/raw/0d6babef04eac378e5a5908568ee0ea280809100/big.txt -O big.txt`\n",
        "contains a larger .txt file.\n",
        "\n",
        "Use this or another .txt file to evaluate whether the GPT-2 model provides better results compared to the previous one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9zFCkk5ERCU"
      },
      "source": [
        "# Finetuning [[1]](https://www.youtube.com/watch?v=quh7z1q7-uc&t=9697s),[[4]](https://www.deeplearning.ai/short-courses/finetuning-large-language-models/),[[5]](https://www.geeksforgeeks.org/fine-tuning-large-language-model-llm/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsY8ctHvQoyF"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1Hil281QlBsY9bjroIitFOPgj9KDrvM4Y\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYHrgvd_EUWr"
      },
      "source": [
        "## Why Finetuning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGohWJvxF1_E"
      },
      "source": [
        "- For example, let's consider a model used for dermatologist problems.\n",
        "  - So you can see in the below figure you might have some symptoms that you input into the model like skin irritation,\n",
        "  redness, itching.\n",
        "  - And the base model which is the general purpose model might just say this is probably acne.\n",
        "  - A model that is fine-tuned on dermatology data however\n",
        "  might take in the same symptoms and be able to give you a much clearer, more specific diagnosis.\n",
        "\n",
        "In addition to learning new information, **fine-tuning can also help steer the model to more consistent outputs or more\n",
        "consistent behavior.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvyeY-1bFz0j"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1B5guwWNmrJPWMyZMZrP8VQI9RR88kP8V\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pGwAeo4GhRV"
      },
      "source": [
        "\n",
        "- For example, you can see the base model here. When you ask it, `what's your first name?`\n",
        "- It might respond with, what's your last name? Because it's seen so much survey data out there of different questions.\n",
        "- **So it doesn't even know that it's supposed to answer that question.**\n",
        "- But a fine-tuned model by contrast, when you ask it, what's your first name?\n",
        "**would be able to respond clearly.**\n",
        "- The answer is `My first name is Sharon`.\n",
        "- In addition to steering the model to more\n",
        "consistent outputs or behavior, _fine tuning can help\n",
        "the model reduce hallucinations, which is a common problem where the model makes stuff up._\n",
        "- **Customizes the model for the tasks and purposes we want.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF3lQfPqHg-Y"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1P9gfkjNAJ7rdeZGKesnRbRLP4nkCv45X\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DWlaAfiRWWO"
      },
      "source": [
        "## Instruction finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pvlqCPSbOzz"
      },
      "source": [
        "One strategy used to improve a model's performance on various tasks is instruction fine-tuning. It's about training the machine learning model using examples that demonstrate how the model should respond to the query. The dataset you use for fine-tuning large language models has to serve the purpose of your instruction. For example, suppose you fine-tune your model to improve its summarization skills. In that case, you should build up a dataset of examples that begin with the instruction to summarize, followed by text or a similar phrase. In the case of translation, you should include instructions like ‚Äútranslate this text.‚Äù These prompt completion pairs allow your model to \"think\" in a new niche way and serve the given specific task.\n",
        "\n",
        "\n",
        "- We saw that pretraining an LLM involves a training procedure where it learns to generate one word at a time\n",
        "- Hence, a pretrained LLM is good at text completion, but it is not good at following instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODxMBk0nbRFf"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=11JJcJW8KYwXMML44qHl7IQa3yIFcAUmW\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcR1Xpg7UeA9"
      },
      "source": [
        "## Some ways to Fine-Tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmXPwcisaUW0"
      },
      "source": [
        "### 1- Self-Supervised Learning for Fine-Tuning\n",
        "\n",
        "**Definition:**\n",
        "- The model is trained on unlabeled text data.\n",
        "\n",
        "- This is the same method used to train base or foundation models like GPT, LLaMA, etc.\n",
        "\n",
        "**How it works:**\n",
        "- The model receives a text sequence and learns to predict the next word or continuation.\n",
        "  - Example: Input: ‚ÄúListen to your‚Äù ‚Üí Output: ‚Äúheart‚Äù\n",
        "\n",
        "- The goal is to learn from the structure of language by predicting what comes next.\n",
        "\n",
        "### 2- Supervised Learning for Fine-Tuning\n",
        "**Definition:**\n",
        "- The model is trained using labeled data consisting of input-output pairs.\n",
        "\n",
        "**How it works:**\n",
        "- Each training example has a clearly defined input and target output.\n",
        "\n",
        "  - Example:\n",
        "    - Input: ‚ÄúWho was the 35th President of the United States?‚Äù\n",
        "    - Output: ‚ÄúJohn F. Kennedy‚Äù\n",
        "\n",
        "\n",
        "- The model learns to produce the correct output given a specific input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8yMdOJQZDJe"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1G_k6m65BBqv2j6vvQKo9VGg9TrpO6JQw\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq_UNLidbS4j"
      },
      "source": [
        "### Preparing a dataset for supervised instruction finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msRAWAgtLcfl"
      },
      "source": [
        "- We use here [a dataset](https://huggingface.co/datasets/knkarthick/dialogsum) from HuggingFace\n",
        "  - DialogSum is a large-scale dialogue summarization dataset, consisting of 13,460 (Plus 100 holdout data for topic generation) dialogues with corresponding manually labeled summaries and topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-24T12:18:34.260019Z",
          "iopub.status.busy": "2025-04-24T12:18:34.259393Z",
          "iopub.status.idle": "2025-04-24T12:18:38.877840Z",
          "shell.execute_reply": "2025-04-24T12:18:38.876742Z",
          "shell.execute_reply.started": "2025-04-24T12:18:34.259999Z"
        },
        "id": "k-1TOuxFfQo6"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "1804862c490443ec8d5483ab5e22508e",
            "2faa67f5eaea43cf840f9eb0e14fe1cd",
            "86de9ac20be945a98dc5d35da58b0c38",
            "8531523af1e740248ba92c2b013fcf4a",
            "fdc5fecf7f184d46b23b8054e4a98c33",
            "b2bf643243d345b58ab5c645e13e03d5",
            "c846c8a9c5fb42348bf63c1f70015d77",
            "14a8c1288cb24b06ac8290655dd1d995",
            "3f22b2cf85744c0682683d60d3d7f226",
            "e71cc0d777ef4e0ea2a26ef737fffc7a",
            "331eaf6f0540489484d574fc16466a76",
            "4d267e2cd6094a21a9b61ff2fffe08d6",
            "b7ad33649c8448868c95eb63dd817dd6",
            "96e5a1ff4e994a64ad56d26f8544a707",
            "58d0887b37dc4327af85745996858307",
            "f705eae5859040179cd4851e8a040702",
            "88d881ed76fa40968bfa12f7caee4cfb",
            "68a6376a3362410c9ea0f31bf8fbe791",
            "128dc3e4d358483d9086cc720ea09ab1",
            "6cda2da935a1429190f83ae0c2f982d0",
            "ce300d403f77496d84f93738a12c6452",
            "c38a80f0e7cc47f7ae5c9045291eb5cc",
            "e707ae9350cd4f99982595c951812706",
            "7967e95316664397a748995546352433",
            "64c3510c2bce4843919c950609b52b9f",
            "1ff48eac9d8147ddb056d0b05916037c",
            "312e76fe844b48dfaba40c8c3581aa5a",
            "66ef48447acb4cd0825e90b7eb814953",
            "bbcc1407c79d4592b6528e62aa9a2810",
            "d4a2d4b402354432bba95dbabafd4df2",
            "222540f79a8140e4b0d7960b23a22e67",
            "09dfae7494f9495091aa18eac9fdbcb5",
            "ad310f1063ab4caa8ab0a5a681cecd73"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-04-24T12:19:12.789344Z",
          "iopub.status.busy": "2025-04-24T12:19:12.789042Z",
          "iopub.status.idle": "2025-04-24T12:19:17.273856Z",
          "shell.execute_reply": "2025-04-24T12:19:17.273345Z",
          "shell.execute_reply.started": "2025-04-24T12:19:12.789289Z"
        },
        "id": "sNr1NM2afWpq",
        "outputId": "35a4207f-2356-4c8d-817a-d01d8d017d84"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1804862c490443ec8d5483ab5e22508e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/12460 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c38a80f0e7cc47f7ae5c9045291eb5cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad310f1063ab4caa8ab0a5a681cecd73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/1500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
        "dataset = load_dataset(huggingface_dataset_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-04-24T12:19:29.474282Z",
          "iopub.status.busy": "2025-04-24T12:19:29.473440Z",
          "iopub.status.idle": "2025-04-24T12:19:29.479134Z",
          "shell.execute_reply": "2025-04-24T12:19:29.478358Z",
          "shell.execute_reply.started": "2025-04-24T12:19:29.474259Z"
        },
        "id": "O5vAOC0HRb1c",
        "outputId": "a4bd7e71-88b4-4240-fa75-1d25a44869eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 12460\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 1500\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1UUM22zRZFu"
      },
      "source": [
        "- The Alpaca like dataset is used for dialogue summarization and is structured as a DatasetDict consisting of three part: `train`, `validation`, and `test`.\n",
        "- Each of them consist of `id`, `dialogue`, `summary` and `topic`.\n",
        "\n",
        "- Let's look at the content of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-04-24T12:19:31.171969Z",
          "iopub.status.busy": "2025-04-24T12:19:31.171252Z",
          "iopub.status.idle": "2025-04-24T12:19:31.178425Z",
          "shell.execute_reply": "2025-04-24T12:19:31.177755Z",
          "shell.execute_reply.started": "2025-04-24T12:19:31.171945Z"
        },
        "id": "ezni_l8ygYzK",
        "outputId": "4375df7f-0819-48c1-8e10-899b4bb04178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Prompt : Summarize the following dialogue  #Person1#: What's wrong with you? Why are you scratching so much?\n",
            "#Person2#: I feel itchy! I can't stand it anymore! I think I may be coming down with something. I feel lightheaded and weak.\n",
            "#Person1#: Let me have a look. Whoa! Get away from me!\n",
            "#Person2#: What's wrong?\n",
            "#Person1#: I think you have chicken pox! You are contagious! Get away! Don't breathe on me!\n",
            "#Person2#: Maybe it's just a rash or an allergy! We can't be sure until I see a doctor.\n",
            "#Person1#: Well in the meantime you are a biohazard! I didn't get it when I was a kid and I've heard that you can even die if you get it as an adult!\n",
            "#Person2#: Are you serious? You always blow things out of proportion. In any case, I think I'll go take an oatmeal bath.  Summary:\n",
            "--------------------------------------------------------------------\n",
            "Human evaluated summary ---->\n",
            "#Person1# thinks #Person2# has chicken pox and warns #Person2# about the possible hazards but #Person2# thinks it will be fine.\n",
            "---------------------------------------------------------------------\n",
            "Topic = chicken pox\n"
          ]
        }
      ],
      "source": [
        "i = 20\n",
        "id = dataset['test'][i]['id']\n",
        "dialogue = dataset['test'][i]['dialogue']\n",
        "summary = dataset['test'][i]['summary']\n",
        "topic = dataset['test'][i]['topic']\n",
        "\n",
        "prompt = f\"Summarize the following dialogue  {dialogue}  Summary:\"\n",
        "\n",
        "print(f\"Input Prompt : {prompt}\")\n",
        "print(\"--------------------------------------------------------------------\")\n",
        "print(\"Human evaluated summary ---->\")\n",
        "print(summary)\n",
        "print(\"---------------------------------------------------------------------\")\n",
        "print(f\"Topic = {topic}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyN9YwAH5wTZ"
      },
      "source": [
        "- To make it work with batches, we add \"padding\" tokens\n",
        "- Tokenized, this looks like as follows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dipDbMJs50EW"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1hWfvchhniiMgtKu08TimdHoP5wKqjxkG\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj5badX-7ClJ"
      },
      "source": [
        "### Instruction finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-24T12:19:36.419003Z",
          "iopub.status.busy": "2025-04-24T12:19:36.418338Z",
          "iopub.status.idle": "2025-04-24T12:20:50.309681Z",
          "shell.execute_reply": "2025-04-24T12:20:50.308605Z",
          "shell.execute_reply.started": "2025-04-24T12:19:36.418983Z"
        },
        "id": "x3tgxUv5Acl7"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#@title Step 1: Install Necessary Libraries\n",
        "!pip install datasets\n",
        "!pip install evaluated\n",
        "!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-24T12:29:00.945323Z",
          "iopub.status.busy": "2025-04-24T12:29:00.945019Z",
          "iopub.status.idle": "2025-04-24T12:29:00.949618Z",
          "shell.execute_reply": "2025-04-24T12:29:00.948952Z",
          "shell.execute_reply.started": "2025-04-24T12:29:00.945285Z"
        },
        "id": "yU_a76_jBwRx"
      },
      "outputs": [],
      "source": [
        "#@title Step 2: Import Libraries\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, GenerationConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "execution": {
          "iopub.execute_input": "2025-04-24T12:29:01.910736Z",
          "iopub.status.busy": "2025-04-24T12:29:01.910374Z",
          "iopub.status.idle": "2025-04-24T12:29:01.915935Z",
          "shell.execute_reply": "2025-04-24T12:29:01.915188Z",
          "shell.execute_reply.started": "2025-04-24T12:29:01.910710Z"
        },
        "id": "SCOvrth9By3N",
        "outputId": "a653db5d-eebf-4c48-e294-0de4b021c03f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Step 3: Configure Device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2025-04-24T12:29:02.129884Z",
          "iopub.status.busy": "2025-04-24T12:29:02.129614Z",
          "iopub.status.idle": "2025-04-24T12:29:02.135637Z",
          "shell.execute_reply": "2025-04-24T12:29:02.134854Z",
          "shell.execute_reply.started": "2025-04-24T12:29:02.129862Z"
        },
        "id": "ghL-8SjWJmdm"
      },
      "outputs": [],
      "source": [
        "#@title Code to avoid unnecessary output\n",
        "import os\n",
        "import warnings\n",
        "import logging\n",
        "from transformers import logging as hf_logging\n",
        "\n",
        "# 1) Set environment variables to silence various logs\n",
        "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"           # show only error messages from Transformers\n",
        "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"    # disable advisory warnings\n",
        "os.environ[\"TQDM_DISABLE\"] = \"1\"                          # disable tqdm progress bars globally\n",
        "\n",
        "# 2) Suppress all warnings from Python\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# 3) Set Hugging Face Transformers logging level to ERROR\n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "hf_logging.set_verbosity_error()                          # suppress internal logging from Transformers\n",
        "hf_logging.disable_progress_bar()                         # disable download/progress bars\n",
        "\n",
        "# 4) Set Hugging Face Hub logs to ERROR level as well\n",
        "logging.getLogger(\"huggingface_hub\").setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-04-24T12:29:02.367581Z",
          "iopub.status.busy": "2025-04-24T12:29:02.367293Z",
          "iopub.status.idle": "2025-04-24T12:29:03.354387Z",
          "shell.execute_reply": "2025-04-24T12:29:03.353783Z",
          "shell.execute_reply.started": "2025-04-24T12:29:02.367563Z"
        },
        "id": "fq04YhYUE9mR",
        "outputId": "7e57b322-ead1-48f9-a3df-b5f0bcda1626"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 12460\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 1500\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Step 4: Load Dataset\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
        "dataset = load_dataset(huggingface_dataset_name)\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-24T12:29:03.392482Z",
          "iopub.status.busy": "2025-04-24T12:29:03.392246Z",
          "iopub.status.idle": "2025-04-24T12:29:03.912251Z",
          "shell.execute_reply": "2025-04-24T12:29:03.911673Z",
          "shell.execute_reply.started": "2025-04-24T12:29:03.392466Z"
        },
        "id": "HJ3ELCQAB1_k"
      },
      "outputs": [],
      "source": [
        "#@title Step 5: Load Pre-trained Model and Tokenizer\n",
        "model_name = \"google/flan-t5-base\"\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-04-24T12:29:03.914786Z",
          "iopub.status.busy": "2025-04-24T12:29:03.914583Z",
          "iopub.status.idle": "2025-04-24T12:29:03.920772Z",
          "shell.execute_reply": "2025-04-24T12:29:03.920203Z",
          "shell.execute_reply.started": "2025-04-24T12:29:03.914771Z"
        },
        "id": "Me_EUnfeB3z7",
        "outputId": "d5c8e97e-79d4-45d0-e84b-59b96c53b4e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable model parameters: 247577856\n",
            "all model parameters: 247577856\n",
            "percentage of trainable model parameters: 100.00%\n"
          ]
        }
      ],
      "source": [
        "#@title Step 6: Define Function to Count Trainable Parameters\n",
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(base_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-04-24T12:29:04.677766Z",
          "iopub.status.busy": "2025-04-24T12:29:04.677158Z",
          "iopub.status.idle": "2025-04-24T12:29:05.712379Z",
          "shell.execute_reply": "2025-04-24T12:29:05.711589Z",
          "shell.execute_reply.started": "2025-04-24T12:29:04.677737Z"
        },
        "id": "oK8sJ2sjB5hz",
        "outputId": "47138da9-8590-41b8-cdbb-06bd20d50717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Prompt : Summarize the following dialogue  #Person1#: What's wrong with you? Why are you scratching so much?\n",
            "#Person2#: I feel itchy! I can't stand it anymore! I think I may be coming down with something. I feel lightheaded and weak.\n",
            "#Person1#: Let me have a look. Whoa! Get away from me!\n",
            "#Person2#: What's wrong?\n",
            "#Person1#: I think you have chicken pox! You are contagious! Get away! Don't breathe on me!\n",
            "#Person2#: Maybe it's just a rash or an allergy! We can't be sure until I see a doctor.\n",
            "#Person1#: Well in the meantime you are a biohazard! I didn't get it when I was a kid and I've heard that you can even die if you get it as an adult!\n",
            "#Person2#: Are you serious? You always blow things out of proportion. In any case, I think I'll go take an oatmeal bath.  Summary:\n",
            "--------------------------------------------------------------------\n",
            "Human evaluated summary ---->\n",
            "#Person1# thinks #Person2# has chicken pox and warns #Person2# about the possible hazards but #Person2# thinks it will be fine.\n",
            "---------------------------------------------------------------------\n",
            "Baseline model generated summary : ---->\n",
            "Person1 is scratching so much that he can't stand it anymore.\n"
          ]
        }
      ],
      "source": [
        "#@title Step 7: Perform Baseline Inference\n",
        "i = 20\n",
        "dialogue = dataset['test'][i]['dialogue']\n",
        "summary = dataset['test'][i]['summary']\n",
        "\n",
        "prompt = f\"Summarize the following dialogue  {dialogue}  Summary:\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "output = tokenizer.decode(base_model.generate(input_ids, max_new_tokens=200)[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Input Prompt : {prompt}\")\n",
        "print(\"--------------------------------------------------------------------\")\n",
        "print(\"Human evaluated summary ---->\")\n",
        "print(summary)\n",
        "print(\"---------------------------------------------------------------------\")\n",
        "print(\"Baseline model generated summary : ---->\")\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "710f00970c4145fabeabb85e3b476f01",
            "1622455b865c43639f722908d397aa20",
            "52faee3420c74a9cab12009922c378fc",
            "ecc874abd4d243e997a209412bdc578a",
            "f2125abba07547aa836b7210da027372",
            "6cf5487d10414b1885b5fecb0b7e0419",
            "8f3377d3bdd4432d8b4ac7439657647d",
            "783b008a9c3b40879a1b13518c9f7976",
            "cf183e5042454e97af0dcc6ceca4f9bb",
            "94a25da76e00487490559b961503eee8",
            "80ee689040d14e7ab1c55bec503e102c",
            "07f5900b9c17446c8cf07593ac31b17d",
            "48d770f0b4a3480ab8735e4d58a798a5",
            "8e507aa0061b41b6b35e131d9b1bf715",
            "a7a4910e4ca04690b25da1d6792b6851",
            "768fde8c5c0242ad81e86d657261c2a5",
            "478eb73fa02b49c1987335978ebb5c56",
            "b6ce2e3a9a2c4b55912b0107e550c60c",
            "1979d34c30744afc8c8a764ee7bc0f91",
            "e998a0d9652b4c9db6965a6eb854a655",
            "5e20b4efba174d598522155db826d822",
            "485a382d74154efd9b8757394fdd907c",
            "adc8114694f747dd95887fd57b69cedd",
            "2ccd9c71fe1b432094d8432b0a5694e2",
            "7f418de7d9a645b3a47f20e52bd61b05",
            "5db24787efc942df8e6d5afe193fb920",
            "74ace47566664ce5b5f183cf78fc1246",
            "367705b5dba84236b005cf1c35b1a49b",
            "79bfdf512b3b438ca9cf76d940351cad",
            "6ef6e06fbfb3439e9157a530de46a8ae",
            "4fe66670d06d4b93b62e2112099d8d62",
            "0ab6acf579a84776873126a8455a9932",
            "a8caa398cb344818a433f65d824154c5",
            "d7ed97b5b4d4471eb3062a37ce8b4330",
            "29555e108eef475da947242eb370b263",
            "d6e6a62ff4154dc0b1ee4e5281755732",
            "3210dcd86f1d413f8bcb21905a8fd6d0",
            "f701c0806e1a46c0aed640645c76584f",
            "e8723325fd32455c83fd0eb8eb5f4033",
            "3bb20ce06dd5448aa23f5bd39f6ab1ae",
            "9d802ea4379c4d438bfb5d197f085c8e",
            "1eacdba67b2d4b339fbbd31f17c0ed3a",
            "08425d8b12f34f4cb169bfb5b6d15bf3",
            "750d503c57604780b64086cfa8697cf4",
            "9ddf1d0dc1c943b48d294ad21a63079f",
            "29be1a9bd39c44e2b6c451a57bc6a6af",
            "7c1887e560c24ec8a9491b2112e73fa6",
            "397d8b091864422bbcb1f2b2f766fffb",
            "0acf7ebc29b543ee98372558362daf19",
            "b5e786693e5c46e38d9ecc13cbf407c6",
            "17ff51c32e344b54a042e82916940e99",
            "39cd9635ecbc462ebe963f334877e7cd",
            "026f36ac4f4e4f9981e84bf3af574006",
            "8c14a0f3ca484f048b83d027e500b038",
            "959d2694b3e64a1d97c2285b2a375bf2"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-04-24T12:29:05.713699Z",
          "iopub.status.busy": "2025-04-24T12:29:05.713486Z",
          "iopub.status.idle": "2025-04-24T12:29:06.286040Z",
          "shell.execute_reply": "2025-04-24T12:29:06.285513Z",
          "shell.execute_reply.started": "2025-04-24T12:29:05.713683Z"
        },
        "id": "qO2-JmifB72L",
        "outputId": "a80e85f9-4a95-4289-bb51-7e7a699eb978"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "710f00970c4145fabeabb85e3b476f01",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/12460 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "485a382d74154efd9b8757394fdd907c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8caa398cb344818a433f65d824154c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "750d503c57604780b64086cfa8697cf4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "959d2694b3e64a1d97c2285b2a375bf2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title Step 8: Tokenize Dataset\n",
        "def tokenize_function(example):\n",
        "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
        "    end_prompt = '\\n\\nSummary: '\n",
        "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
        "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "    return example\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary'])\n",
        "\n",
        "\n",
        "tokenized_datasets[\"train\"] = tokenized_datasets[\"train\"].filter(lambda example, idx: idx % 100 == 0, with_indices=True)\n",
        "tokenized_datasets[\"validation\"] = tokenized_datasets[\"validation\"].filter(lambda example, idx: idx % 100 == 0, with_indices=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nSe8Igmons4"
      },
      "source": [
        "#### Parameter-efficient fine-tuning (PEFT)\n",
        "Training a language model is a computationally intensive task. For a full LLM fine-tuning, you need memory not only to store the model, but also the parameters that are necessary for the training process. Your computer might be able to handle the model weights, but allocating memory for optimizing states, gradients, and forward activations during the training process is a challenging task. Simple hardware cannot handle this amount of hurdle. This is where PEFT is crucial. While full LLM fine-tuning updates every model's weight during the supervised learning process, **PEFT methods only update a small set of parameters**. This transfer learning technique chooses specific model components and \"freezes\" the rest of the parameters. The result is logically having a much smaller number of parameters than in the original model (in some cases, just 15-20% of the original weights; LoRA can reduce the number of trainable parameters by 10,000 times). This makes memory requirements much more manageable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_23KsAQ7oGDY"
      },
      "source": [
        "#### Introduction to LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_SmfYAKoK4A"
      },
      "source": [
        "- Suppose we have a large weight matrix $W$ for a given layer\n",
        "- During backpropagation, we learn a $\\Delta W$ matrix, which contains information on how much we want to update the original weights to minimize the loss function during training\n",
        "- In regular training and finetuning, the weight update is defined as follows:\n",
        "\n",
        "$$W_{\\text{updated}} = W + \\Delta W$$\n",
        "\n",
        "- The LoRA method proposed by [Hu et al.](https://arxiv.org/abs/2106.09685) offers a more efficient alternative to computing the weight updates $\\Delta W$ by learning an approximation of it, $\\Delta W \\approx AB$.\n",
        "- In other words, in LoRA, we have the following, where $A$ and $B$ are two small weight matrices:\n",
        "\n",
        "$$W_{\\text{updated}} = W + AB$$\n",
        "\n",
        "- The figure below illustrates these formulas for full finetuning and LoRA side by side"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUzUDt7gpScg"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1enMlk4mLyqiccwRI0zOMBLKQ_GcJt7n2\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCNs_x5kpRce"
      },
      "source": [
        "- If you paid close attention, the full finetuning and LoRA depictions in the figure above look slightly different from the formulas I have shown earlier\n",
        "- That's due to the distributive law of matrix multiplication: we don't have to add the weights with the updated weights but can keep them separate\n",
        "- For instance, if $x$ is the input data, then we can write the following for regular finetuning:\n",
        "\n",
        "$$x (W+\\Delta W) = x W + x \\Delta W$$\n",
        "\n",
        "- Similarly, we can write the following for LoRA:\n",
        "\n",
        "$$x (W+A B) = x W + x A B$$\n",
        "\n",
        "- The fact that we can keep the LoRA weight matrices separate makes LoRA especially attractive\n",
        "- In practice, this means that we don't have to modify the weights of the pretrained model at all, as we can apply the LoRA matrices on the fly\n",
        "- After setting up the dataset and loading the model, we will implement LoRA in the code to make these concepts less abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-04-24T12:29:10.162401Z",
          "iopub.status.busy": "2025-04-24T12:29:10.162113Z",
          "iopub.status.idle": "2025-04-24T12:29:10.242698Z",
          "shell.execute_reply": "2025-04-24T12:29:10.242118Z",
          "shell.execute_reply.started": "2025-04-24T12:29:10.162377Z"
        },
        "id": "K7N4AOgJCFBr",
        "outputId": "99a8a522-799c-4640-a8bc-2ccc988039c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable model parameters: 884736\n",
            "all model parameters: 248462592\n",
            "percentage of trainable model parameters: 0.36%\n"
          ]
        }
      ],
      "source": [
        "#@title Step 9: Apply PEFT with LoRA Configuration\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        ")\n",
        "\n",
        "peft_model_train = get_peft_model(base_model, lora_config)\n",
        "print(print_number_of_trainable_model_parameters(peft_model_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-24T12:29:10.981461Z",
          "iopub.status.busy": "2025-04-24T12:29:10.981172Z",
          "iopub.status.idle": "2025-04-24T12:29:11.007518Z",
          "shell.execute_reply": "2025-04-24T12:29:11.006945Z",
          "shell.execute_reply.started": "2025-04-24T12:29:10.981440Z"
        },
        "id": "nx5WdPDxCHFc"
      },
      "outputs": [],
      "source": [
        "#@title Step 10: Define Training Arguments\n",
        "output_dir = \"./peft-dialogue-summary-training\"\n",
        "peft_training_args = TrainingArguments(\n",
        "    report_to=\"tensorboard\",                   # report logs to TensorBoard\n",
        "    logging_dir=\"./tb_logs\",                   # directory for saving TensorBoard logs\n",
        "    logging_strategy=\"epoch\",                  # log every few steps\n",
        "    logging_steps=10,                          # log every 10 steps\n",
        "    output_dir=output_dir,\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3,\n",
        "    num_train_epochs=5,\n",
        "    eval_strategy=\"epoch\",                     # evaluate the model at the end of each epoch\n",
        "    disable_tqdm=False,                        # enable progress bars\n",
        "    log_level=\"info\"                           # log level set to 'info' (to log train loss and other details)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2025-04-24T12:29:11.547257Z",
          "iopub.status.busy": "2025-04-24T12:29:11.546992Z",
          "iopub.status.idle": "2025-04-24T12:30:40.802755Z",
          "shell.execute_reply": "2025-04-24T12:30:40.802184Z",
          "shell.execute_reply.started": "2025-04-24T12:29:11.547236Z"
        },
        "id": "hyuz3U0ECIkf",
        "outputId": "65d37917-a8d0-461f-cd8d-7f208f4f926a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "***** Running training *****\n",
            "  Num examples = 125\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 80\n",
            "  Number of trainable parameters = 884,736\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "***** Running training *****\n",
            "  Num examples = 125\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 8\n",
            "  Training with DataParallel so batch size has been adjusted to: 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 160\n",
            "  Number of trainable parameters = 884,736\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [160/160 02:40, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>14.845300</td>\n",
              "      <td>2.380888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.432200</td>\n",
              "      <td>0.234731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.351500</td>\n",
              "      <td>0.139262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.244900</td>\n",
              "      <td>0.122611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.221200</td>\n",
              "      <td>0.119260</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5\n",
            "  Batch size = 8\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5\n",
            "  Batch size = 8\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5\n",
            "  Batch size = 8\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./peft-dialogue-summary-training/checkpoint-160\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/config.json\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=160, training_loss=3.419025328755379, metrics={'train_runtime': 161.6771, 'train_samples_per_second': 3.866, 'train_steps_per_second': 0.99, 'total_flos': 429672038400000.0, 'train_loss': 3.419025328755379, 'epoch': 5.0})"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Step 11: Train the Model\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model_train,\n",
        "    args=peft_training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        ")\n",
        "peft_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "execution": {
          "iopub.execute_input": "2025-04-24T12:30:40.804240Z",
          "iopub.status.busy": "2025-04-24T12:30:40.804049Z",
          "iopub.status.idle": "2025-04-24T12:30:40.977082Z",
          "shell.execute_reply": "2025-04-24T12:30:40.976450Z",
          "shell.execute_reply.started": "2025-04-24T12:30:40.804224Z"
        },
        "id": "i_dE3_zeJywN",
        "outputId": "c5a564f7-8d06-4d74-bff5-e3c59d41bdfc"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history = peft_trainer.state.log_history\n",
        "df = pd.DataFrame(history)\n",
        "\n",
        "train_df = df[df[\"loss\"].notna()]\n",
        "eval_df  = df[df[\"eval_loss\"].notna()]\n",
        "\n",
        "plt.plot(train_df[\"epoch\"], train_df[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(eval_df[\"epoch\"],  eval_df[\"eval_loss\"], label=\"eval_loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Train vs Eval Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-04-24T12:30:40.978013Z",
          "iopub.status.busy": "2025-04-24T12:30:40.977791Z",
          "iopub.status.idle": "2025-04-24T12:30:41.202012Z",
          "shell.execute_reply": "2025-04-24T12:30:41.201265Z",
          "shell.execute_reply.started": "2025-04-24T12:30:40.977993Z"
        },
        "id": "w43R5ODS7H-6",
        "outputId": "0ccb0a0a-c17e-439d-8388-82cdada8d32f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/config.json\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./peft-dialogue-summary-checkpoint-local/tokenizer_config.json\n",
            "Special tokens file saved in ./peft-dialogue-summary-checkpoint-local/special_tokens_map.json\n",
            "Copy vocab file to ./peft-dialogue-summary-checkpoint-local/spiece.model\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "#@title Step 12: Save the Fine-Tuned Model\n",
        "peft_model_path = \"./peft-dialogue-summary-checkpoint-local\"\n",
        "peft_trainer.model.save_pretrained(peft_model_path)\n",
        "tokenizer.save_pretrained(peft_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-04-24T12:30:59.468149Z",
          "iopub.status.busy": "2025-04-24T12:30:59.467874Z",
          "iopub.status.idle": "2025-04-24T12:31:03.712902Z",
          "shell.execute_reply": "2025-04-24T12:31:03.712226Z",
          "shell.execute_reply.started": "2025-04-24T12:30:59.468129Z"
        },
        "id": "iG3qrqz2_k_4",
        "outputId": "0376066e-5e43-400e-999f-09100cdb5cfb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/config.json\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/model.safetensors\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/flan-t5-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Prompt : Summarize the following dialogue  #Person1#: What's wrong with you? Why are you scratching so much?\n",
            "#Person2#: I feel itchy! I can't stand it anymore! I think I may be coming down with something. I feel lightheaded and weak.\n",
            "#Person1#: Let me have a look. Whoa! Get away from me!\n",
            "#Person2#: What's wrong?\n",
            "#Person1#: I think you have chicken pox! You are contagious! Get away! Don't breathe on me!\n",
            "#Person2#: Maybe it's just a rash or an allergy! We can't be sure until I see a doctor.\n",
            "#Person1#: Well in the meantime you are a biohazard! I didn't get it when I was a kid and I've heard that you can even die if you get it as an adult!\n",
            "#Person2#: Are you serious? You always blow things out of proportion. In any case, I think I'll go take an oatmeal bath.  Summary:\n",
            "--------------------------------------------------------------------\n",
            "Human evaluated summary ---->\n",
            "#Person1# thinks #Person2# has chicken pox and warns #Person2# about the possible hazards but #Person2# thinks it will be fine.\n",
            "---------------------------------------------------------------------\n",
            "Baseline model generated summary : ---->\n",
            "Person1 is scratching so much that he can't stand it anymore.\n",
            "---------------------------------------------------------------------\n",
            "Peft model generated summary : ---->\n",
            "#Person1# thinks he has chicken pox. #Person2# thinks he has chicken pox.\n"
          ]
        }
      ],
      "source": [
        "#@title Step 13: Load and Test Fine-Tuned Model\n",
        "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
        "peft_model = PeftModel.from_pretrained(peft_model_base, peft_model_path, is_trainable=False)\n",
        "\n",
        "\n",
        "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Input Prompt : {prompt}\")\n",
        "print(\"--------------------------------------------------------------------\")\n",
        "print(\"Human evaluated summary ---->\")\n",
        "print(summary)\n",
        "print(\"---------------------------------------------------------------------\")\n",
        "print(\"Baseline model generated summary : ---->\")\n",
        "print(output)\n",
        "print(\"---------------------------------------------------------------------\")\n",
        "print(\"Peft model generated summary : ---->\")\n",
        "print(peft_model_text_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFotXh9ugid7"
      },
      "source": [
        "## **Exercise 4: Collect baseline and fine-tuned model responses**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkIy6s3ygid7"
      },
      "source": [
        "#### Step 1:\n",
        "- In this excercise, we are collecting the model responses on the test dataset so that we can evaluate them later\n",
        "- Starting with the original model before finetuning\n",
        "- Then use the base_model function to generate the responses for the test data\n",
        "\n",
        "#### Step 2:\n",
        "- Repeat the steps from the previous exercise but this time collect the responses of the finetuned model\n",
        "- Use the PeftModel function to generate the responses for the test data\n",
        "\n",
        "#### Step 3:\n",
        "- Save the resulting `test_data` dictionary as `baseline_and_finetuned_results.json`\n",
        "\n",
        "\n",
        "- Your json file should look like this\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"index\": 0,\n",
        "    \"prompt\": \"Summarize the following dialogue  #Person1#: Ms. Dawson, I need you to take a dictation for me.\\n#Person2#: Yes, sir...\\n#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\\n#Person2#: Yes, sir. Go ahead.\\n#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.....  Summary:\",\n",
        "    \"human_summary\": \"Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\",\n",
        "    \"model_output_base\": \"The memo will go out to all employees by this afternoon.\",\n",
        "    \"model_output_finetuning\": \"This memo should go out as an intra-office memo to all employees by this afternoon.\"\n",
        "},\n",
        "{\n",
        "    \"index\": 1,\n",
        "    \"prompt\": \"Summarize the following dialogue  #Person1#: Ms. Dawson, I need you to take a dictation for me.\\n#Person2#: Yes, sir...\\n#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\\n#Person2#: Yes, sir. Go ahead.\\n#Person1#: Attention all staff...  Summary:\",\n",
        "    \"human_summary\": \"In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\",\n",
        "    \"model_output_base\": \"The memo will go out to all employees by this afternoon.\",\n",
        "    \"model_output_finetuning\": \"This memo should go out as an intra-office memo to all employees by this afternoon.\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgAyMKXvgid7"
      },
      "source": [
        "# Benchmark Evaluation [[1]](https://www.youtube.com/watch?v=quh7z1q7-uc&t=9697s), [[6]](https://aclanthology.org/W04-1013.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRWXBcY4gid7"
      },
      "source": [
        "- We finetuned the LLM; now, we evaluate it using benchmark methods\n",
        "\n",
        "- **Examples of some evaluation techniques:**\n",
        "\n",
        "1. **MMLU**\n",
        "\n",
        "   **What is MMLU?**\n",
        "     - MMLU (Massive Multitask Language Understanding) is a benchmark that evaluates how well a model understands and reasons across various subjects.\n",
        "       - The format is usually: Question + 4 choices ‚Üí Select the correct answer\n",
        "       - Subjects include: History, Physics, Mathematics, Medicine, Law, and more.\n",
        "       - It is a classification/multiple-choice QA task, not a text generation task.\n",
        "       - The primary evaluation metric is Accuracy (% of correct answers).\n",
        "\n",
        "   <img src=\"https://drive.google.com/uc?export=view&id=1aAbQBmMKohJO-HVaeJBi9XfZHISaD_A_\" width=\"600\">\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1e5YaA5EtBH0R9OgYsEtU2_kSZT6jkv4R\" width=\"600\">\n",
        "\n",
        "\n",
        "2. **AlpacaEval**\n",
        "\n",
        "    **What is AlpacaEval?**\n",
        "    - AlpacaEval is a system developed to evaluate the quality of generative responses produced by large language models (LLMs).\n",
        "    - Instead of using traditional metrics, it leverages another LLM (e.g., GPT-4) to compare responses in a pairwise manner and determine which model performed better.\n",
        "    \n",
        "    - For example:\n",
        "      - The data might look like this:\n",
        "      - Prompt: \"Explain the theory of relativity.\"\n",
        "      - Model A response\n",
        "\n",
        " <img src=\"https://drive.google.com/uc?export=view&id=1VeCYU31Sy9ZW9E-kNLoVo5j6Vfh6MI81\" width=\"600\">\n",
        "\n",
        "3. **LMSYS Chatbot Arena**\n",
        "\n",
        "  - Chatbot Arena (formerly LMSYS Chatbot Arena) is an open, community-supported platform built to evaluate the performance of large language models (LLMs) with human feedback.\n",
        "\n",
        " <img src=\"https://drive.google.com/uc?export=view&id=1_DktiAcwAgenlOsa1ZcL9KkTpeZqAjWI\" width=\"1000\">\n",
        "\n",
        "\n",
        "4. **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\n",
        "\n",
        "   ROUGE (Recall-Oriented Understudy for Gisting Evaluation), is a set of metrics and a software package specifically designed for **evaluating automatic summarization**, but that can be also used for machine translation. The metrics compare an automatically produced summary or translation against reference (high-quality and human-produced) summaries or translations. **Since there is a summarization process in our data set, we will use this metric.**\n",
        "\n",
        "-  It essentially compares the following:\n",
        "\n",
        "   - The generated text (e.g., the model‚Äôs output)\n",
        "\n",
        "   - The reference text (e.g., a human-written gold summary)\n",
        "\n",
        " - And it evaluates based on overlapping n-grams (word groups), word sequences, and similar metrics between the two.\n",
        "\n",
        "\n",
        "- **Types of ROUGE:**\n",
        " - ROUGE-1:\n",
        "   ‚Äì Counts overlapping individual words.\n",
        "   ‚Äì How many of the same words are used?\n",
        "\n",
        " - ROUGE-2:\n",
        "   ‚Äì Counts overlapping bigrams (two-word combinations).\n",
        "   ‚Äì How much do phrases like ‚Äúdeep learning‚Äù match?\n",
        "\n",
        " - ROUGE-L:\n",
        "  ‚Äì Looks at the Longest Common Subsequence (LCS) in terms of length.\n",
        "  ‚Äì Takes word order into account.\n",
        "  ‚Äì Are the words not just the same, but also in the same order?\n",
        "\n",
        "\n",
        "Click the [link](https://aclanthology.org/W04-1013.pdf) for more information\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LptCtz5gid7"
      },
      "source": [
        "## **Exercise 5: Benchmark Evaluation with ROUGE**\n",
        "\n",
        "You will perform evaluation using the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) method by using the `baseline_and_finetuned_results.json` file you saved in the previous example.\n",
        "\n",
        "## Step 1:\n",
        "Install the required library for ROUGE = `!pip install rouge-score`\n",
        "\n",
        "## Step 2:\n",
        "Read the JSON file using the `with open()` command.\n",
        "\n",
        "## Step 3:\n",
        "Define the `scorer`. Use `RougeScorer` with `rouge1`, `rouge2`, and `rougeL`.\n",
        "\n",
        "## Step 4:\n",
        "For each index in the JSON file, extract the parts as shown below and compute `base_score` and `finetuned_score`. Then, interpret the results.\n",
        "```python\n",
        "    human = item['human_summary']\n",
        "    base = item['model_output_base']\n",
        "    finetuned = item['model_output_finetuning']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgBnRT9JtedY"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTcq1Jsnto2o"
      },
      "source": [
        "- [1] [Sebastian Raschka](https://www.youtube.com/watch?v=quh7z1q7-uc&t=9697s)\n",
        "- [2] [DeepLearning.ai Tokenization](https://www.deeplearning.ai/short-courses/retrieval-optimization-from-tokenization-to-vector-quantization/)\n",
        "- [3] [DeepLearning.ai Pretraining LLMs](https://www.deeplearning.ai/short-courses/pretraining-llms/)\n",
        "- [4] [DeepLearning.ai Finetuning](https://www.deeplearning.ai/short-courses/finetuning-large-language-models/)\n",
        "- [5] [Fine Tuning](https://www.geeksforgeeks.org/fine-tuning-large-language-model-llm/)\n",
        "- [6] [aclanthology](https://aclanthology.org/W04-1013.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlNI86MAtd5m"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "MYU_Zj6RvKUy",
        "BFTDjwcp03fc",
        "ozZ47m0odFyw",
        "ED6KvSo8tSd4",
        "sstsUJgpEsxw",
        "q9zFCkk5ERCU",
        "zgAyMKXvgid7"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 7241184,
          "sourceId": 11546872,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31011,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}